{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciclo KDD \n",
    "## Aplicado a Predicción de Demanda\n",
    "\n",
    "**Objetivo:** Aplicar el ciclo completo de Knowledge Discovery in Databases (KDD) de forma práctica para construir un modelo predictivo simple de la demanda diaria de croissants en una panadería en base a datos sintéticos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <div style=\"text-align: center; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\"El Buen Croissant\".</div>\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/medialuna.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n",
    "# <div style=\"text-align: center; text-shadow: 2px 2px 4px rgba(31, 2, 63, 0.3);\">... Quiere saber cómo mejorar sus ventas.</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inmersión en Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones principales\n",
    "\n",
    "# Librerías básicas\n",
    "import os\n",
    "import pickle  # Para guardar el DataFrame\n",
    "import subprocess\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# Análisis de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.parquet import to_parquet\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Estadísticas y modelado\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeRegressor,\n",
    "    export_text,\n",
    "    export_graphviz,\n",
    "    plot_tree,\n",
    "    _tree\n",
    ")\n",
    "\n",
    "# Otras librerías\n",
    "import holidays\n",
    "import shap\n",
    "\n",
    "# Funciones propias\n",
    "from funciones.describe_vars import describe_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot') #https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5194",
   "metadata": {},
   "source": [
    "### Detalle: Configuración de Visualización y Manejo de Errores\n",
    "\n",
    "1. **Configuración Visual**: Se pueden definir los estilos y parámetros para las visualizaciones usando seaborn y matplotlib, asegurando una presentación consistente y profesional de nuestros gráficos.\n",
    "\n",
    "2. **Manejo de Errores**: Se han implementado funciones de utilidad para el manejo de errores y validación de datos, lo que nos permitirá tener un mejor control sobre posibles problemas durante el análisis.\n",
    "\n",
    "Estas configuraciones van a afectar todo el resto del flujo de trabajo en el archivo. Se pueden agregar más, o especificar luego en cada celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3e4b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- abrimos el csv ---\n",
    "DATA_FILE = 'panaderia_croissant_synthetic.csv'\n",
    "\n",
    "# csv a data frame df_raw\n",
    "df_raw = pd.read_csv(DATA_FILE)\n",
    "\n",
    "\n",
    "# Es mejor usar la siguiente celda que implementa la carga con PKL o parquet, usamos pkl por ahora. \n",
    "# para mejorar la eficiencia en ejecuciones posteriores. Vamos a hacerlo correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_1.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga de Datos y Guardado en PKL ---\n",
    "DATA_FILE = 'panaderia_croissant_synthetic.csv'\n",
    "PKL_FILE = 'croissant_demand_raw.pkl'\n",
    "\n",
    "try:\n",
    "    # Intentar cargar PKL primero\n",
    "    with open(PKL_FILE, 'rb') as f:\n",
    "        df_raw = pickle.load(f)\n",
    "    print(f\"DataFrame cargado desde '{PKL_FILE}' exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Archivo PKL no encontrado. Intentando cargar CSV...\")\n",
    "    try:\n",
    "        # Si no existe PKL, cargar CSV\n",
    "        df_raw = pd.read_csv(DATA_FILE)\n",
    "        print(f\"Archivo '{DATA_FILE}' cargado exitosamente.\")\n",
    "        \n",
    "        # Guardar en PKL para futuras ejecuciones\n",
    "        with open(PKL_FILE, 'wb') as f:\n",
    "            pickle.dump(df_raw, f)\n",
    "        print(f\"DataFrame guardado en '{PKL_FILE}' para uso futuro.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo '{DATA_FILE}' no se encontró.\")\n",
    "        raise SystemExit(\"No se puede continuar sin datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga con try-except\n",
    "\n",
    "El bloque try-except es una estructura de control que nos permite manejar posibles errores:\n",
    "\n",
    "1. El código primero intenta (try) cargar el archivo PKL que es más rápido y eficiente:\n",
    "   - Si el archivo existe, lo carga y continúa normalmente\n",
    "   - Si el archivo no existe, Python genera un error FileNotFoundError\n",
    "\n",
    "2. Cuando ocurre el error, el código salta al bloque except:\n",
    "   - Aquí tenemos un plan B: intentar cargar el CSV original\n",
    "   - Usamos otro try-except anidado por si tampoco existe el CSV\n",
    "   - Si el CSV existe, lo carga y además lo guarda como PKL para la próxima vez\n",
    "   - Si el CSV tampoco existe, termina el programa con un mensaje de error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Qué vamos a hacer? \n",
    "\n",
    "\n",
    "---\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/medialuna.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n",
    "---\n",
    "\n",
    "\n",
    "#### 1. Carga de datos, manipulación y preprocesamiento\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Transformación de variables (iteración 1)\n",
    "#### 3. Análisis exploratorio (descriptivas básicas)\n",
    "#### 2.1 Transformación de variables (iteración 2)\n",
    "#### 3.1 Análisis exploratorio (correlaciones, regresiones, ANOVA)\n",
    "#### 3.2 Intuiciones exploratorias\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Definición de predictores (X) y objetivo (y)\n",
    "#### 5. División temporal Train/Test\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Modelado predictivo: qué modelos para qué problemas\n",
    "#### 6. Modelado predictivo: entrenamiento, validación, comparación\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Elección del mejor modelo (Supervised Machine Learning with Decision Trees)\n",
    "#### 8. Despliegue del modelo seleccionado\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. Validación del modelo\n",
    "#### 10. Interpretación del modelo\n",
    "\n",
    "\n",
    "    ... además... vamos a seguir aprendiendo python, pandas y herramientas/técnicas de análisis \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_2.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información básica del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga desde PKL y Descripción Inicial ---\n",
    "with open(PKL_FILE, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "# Dimensiones del DataFrame:\n",
    "# Filas: {df.shape[0]}, Columnas: {df.shape[1]}\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de datos por columna:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras 3 filas del dataset crudo:\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustar el DataFrame a Serie Temporal\n",
    "\n",
    "Este paso es importante para trabajar con series temporales:\n",
    "\n",
    "1. Primero, convertiremos la columna 'Fecha' al tipo datetime de pandas\n",
    "2. Luego, estableceremos esta columna como el índice del DataFrame\n",
    "3. Finalmente, exploraremos las nuevas capacidades que nos brinda tener un índice temporal:\n",
    "   - Verificaremos el rango de fechas en nuestros datos\n",
    "   - Identificaremos el período que abarcan los datos\n",
    "   - Confirmaremos la frecuencia de las observaciones\n",
    "\n",
    "Esta transformación nos permitirá:\n",
    "- Realizar análisis temporales más sofisticados\n",
    "- Utilizar funcionalidades específicas de series temporales\n",
    "- Facilitar la visualización de tendencias y patrones a lo largo del tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tipo de la columna Fecha:\", df['Fecha'].dtype)\n",
    "print(\"Primeras 3 fechas:\", df['Fecha'].head(3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar una muestra del estado anterior\n",
    "# Mostramos el tipo de la columna Fecha y las primeras 3 fechas antes de la transformación\n",
    "display(pd.DataFrame({\n",
    "    'Tipo de la columna Fecha': [df['Fecha'].dtype],\n",
    "    'Primeras 3 fechas': [df['Fecha'].head(3).tolist()]\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir a datetime y establecer como índice\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "df.set_index('Fecha', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información después de la transformación\n",
    "print(\"Tipo del índice:\", df.index.dtype)\n",
    "print(\"Primeras 3 fechas del índice:\", df.index[:3].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importancia del uso del Índice Temporal\n",
    "\n",
    "La transformación realizada, convirtiendo la columna 'Fecha' a tipo datetime y estableciéndola como índice del DataFrame, es fundamental por varias razones:\n",
    "\n",
    "1. **Funcionalidades Específicas de Series Temporales**:\n",
    "   - Permite acceder a componentes temporales como año, mes, día, etc.\n",
    "   - Facilita el cálculo de diferencias temporales y rangos de fechas\n",
    "   - Habilita el uso de funciones específicas para análisis de series temporales\n",
    "\n",
    "2. **Mejora en la Manipulación de Datos**:\n",
    "   - Permite realizar resampling (cambios en la frecuencia de los datos)\n",
    "   - Facilita la selección y filtrado por períodos específicos\n",
    "\n",
    "3. **Ventajas para la Visualización**:\n",
    "   - Los gráficos temporales se generan automáticamente con el eje X correctamente formateado\n",
    "   - Facilita la identificación de patrones, tendencias y estacionalidad\n",
    "   - Mejora la interpretación de la evolución temporal de las variables\n",
    "\n",
    "4. **Validación y Calidad de Datos**:\n",
    "   - Permite detectar gaps en la serie temporal\n",
    "   - Facilita la identificación de frecuencias en los datos\n",
    "   - Ayuda a verificar la consistencia temporal de las observaciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capacidades adicionales con el índice datetime\n",
    "print(\"\\nInformación temporal:\")\n",
    "print(\"Año más antiguo:\", df.index.year.min())\n",
    "print(\"Año más reciente:\", df.index.year.max()) \n",
    "print(\"Rango de fechas:\", df.index.max() - df.index.min())\n",
    "print(\"Frecuencia de los datos:\", pd.infer_freq(df.index))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración: Análisis descriptivo\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estadísticas descriptivas básicas\n",
    "print(\"\\nEstadísticas Descriptivas Básicas:\")\n",
    "print(df.describe().round(2).T)\n",
    "# round() redondea los valores a 2 decimales para una mejor visualización\n",
    "\n",
    "# .T transpone el DataFrame, convirtiendo las filas en columnas y viceversa\n",
    "# Esto facilita la lectura de las estadísticas descriptivas, mostrando cada variable en una fila\n",
    "# en lugar de en columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.describe().round(2).T)\n",
    "display(df.describe().round(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verificar duplicados\n",
    "print(\"\\nNúmero de filas duplicadas:\", df.duplicated().sum())\n",
    "print(\"Número de índices (fechas) duplicados:\", df.index.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_vars(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Descriptivo Adicional (sobre t)\n",
    "\n",
    "En esta sección realizaremos un análisis descriptivo avanzado que incluirá:\n",
    "\n",
    "- Análisis de ventas por día de la semana\n",
    "- Análisis de ventas por mes \n",
    "- Matriz de correlación entre variables numéricas\n",
    "- Comparación de ventas con y sin promoción\n",
    "- Distribución de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bccd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estadísticas por día de la semana\n",
    "print(\"\\nEstadísticas de Cantidad_Vendida por día de la semana:\")\n",
    "display(df.groupby('Dia_Semana')['Cantidad_Vendida'].agg(['count', 'mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estadísticas por mes\n",
    "print(\"\\nEstadísticas de Cantidad_Vendida por mes:\")\n",
    "display(df.groupby('Mes')['Cantidad_Vendida'].agg(['count', 'mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Estadísticas para días con y sin promoción\n",
    "print(\"\\nComparación de ventas con y sin promoción:\")\n",
    "display(df.groupby('Promocion_Croissant')['Cantidad_Vendida'].agg(['count', 'mean', 'std', 'min', 'max']).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de variables categóricas\n",
    "print(\"\\nDistribución de variables categóricas:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nDistribución de {col}:\")\n",
    "    display(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_vars(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Análisis de correlaciones\n",
    "print(\"\\nMatriz de correlación para variables numéricas:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr().round(2)\n",
    "display(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentarios de \"np.number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Análisis de correlaciones\n",
    "print(\"\\nMatriz de correlación para variables numéricas:\")\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# select_dtypes() es un método de pandas que filtra columnas según su tipo de dato\n",
    "# include=[np.number] especifica que solo queremos columnas numéricas (int64, float64)\n",
    "# .columns devuelve los nombres de las columnas filtradas\n",
    "# En este caso seleccionará: Mes, Anio, Dia_Anio, Es_Feriado, Temperatura_Max_Prevista,\n",
    "# Promocion_Croissant, Precio_Nuestro, Precio_Competencia, Cantidad_Vendida\n",
    "# Ejemplos de select_dtypes() con diferentes tipos:\n",
    "\n",
    "# Para seleccionar columnas numéricas (como lo usamos arriba):\n",
    "# df.select_dtypes(include=[np.number]).columns\n",
    "# Seleccionaría: Mes, Anio, Dia_Anio, Es_Feriado, etc.\n",
    "\n",
    "# Para seleccionar columnas de texto (strings):\n",
    "# df.select_dtypes(include=['object']).columns\n",
    "# Seleccionaría: 'Dia_Semana' (ej: \"lunes\", \"martes\")\n",
    "\n",
    "# Para seleccionar fechas:\n",
    "# df.select_dtypes(include=['datetime64']).columns \n",
    "# Seleccionaría columnas de fecha si 'Fecha' fuera datetime\n",
    "# Nota: En nuestro caso 'Fecha' es tipo object porque no se convirtió a datetime\n",
    "\n",
    "# Para seleccionar múltiples tipos a la vez:\n",
    "# df.select_dtypes(include=['object', 'datetime64', np.number]).columns\n",
    "\n",
    "# Para excluir ciertos tipos:\n",
    "# df.select_dtypes(exclude=['object']).columns\n",
    "# Excluiría 'Dia_Semana' y 'Fecha' (si son object)\n",
    "\n",
    "\n",
    "correlation_matrix = df[numeric_cols].corr().round(2)\n",
    "\n",
    "display(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variantes simples de np.number\n",
    "\n",
    "\n",
    "int_cols = df.select_dtypes(include=['int64']).columns\n",
    "display(int_cols)\n",
    "\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "display(float_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el heatmap de correlaciones con fuentes más pequeñas y fondo blanco\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")  # Aplicar estilo de seaborn\n",
    "sns.set_context(\"notebook\", font_scale=1.1)  # Ajustar contexto de seaborn\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,  # Muestra los valores numéricos\n",
    "            cmap='coolwarm',  # Esquema de colores\n",
    "            center=0.1,  # Centra el mapa de colores en 0\n",
    "            fmt='.2f',  # Formato de los números (2 decimales)\n",
    "            square=True,  # Hace que las celdas sean cuadradas\n",
    "            cbar_kws={'label': 'Coeficiente de Correlación'},\n",
    "            annot_kws={'size': 8},  # Tamaño de fuente más pequeño para los valores\n",
    "            linewidths=0.9)  # Líneas más finas entre celdas\n",
    "\n",
    "plt.title('Matriz de Correlación', fontsize=12)  # Ajustar tamaño del título\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respecto de la correlación 1 entre mes y año. \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Mes'], df['Dia_Anio'], alpha=0.5)\n",
    "plt.title('Relación entre Mes y Día del Año')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Día del Año')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora en v2!\n",
    "\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración: EDA y Preprocesamiento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Objetivo: Entender patrones, relaciones, estacionalidad, tendencia y posibles problemas en los datos. Esto guiará la transformación y el modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Estático y Dinámico\n",
    "\n",
    "---\n",
    "\n",
    "### Análisis Estático: Visualizaciones y Distribuciones\n",
    "\n",
    "1. Distribuciones y Estadísticas Básicas\n",
    "   - Histogramas y boxplots de la variable objetivo (Cantidad_Vendida)\n",
    "   - Estadísticas descriptivas: media, mediana, desviación estándar, etc.\n",
    "   - Identificación de valores atípicos y su posible origen\n",
    "\n",
    "2. Análisis por Variables Categóricas\n",
    "   - Ventas promedio por día de la semana (barplot)\n",
    "   - Ventas mensuales agregadas (barplot)\n",
    "   - Comparación de ventas en días festivos vs no festivos (boxplot)\n",
    "   - Impacto de promociones en las ventas (boxplot comparativo)\n",
    "\n",
    "3. Relaciones entre Variables Numéricas\n",
    "   - Matriz de correlación con heatmap\n",
    "   - Scatterplots de variables relevantes vs Cantidad_Vendida\n",
    "   - Relación entre precios (nuestro vs competencia) y ventas\n",
    "   - Efecto de la temperatura en las ventas\n",
    "\n",
    "4. Análisis de Composición\n",
    "   - Proporción de ventas por temporada\n",
    "   - Distribución de ventas según rangos de precio\n",
    "   - Participación por día de la semana\n",
    "   - Comparación interanual de patrones\n",
    "   ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas Clave para el Análisis Visual de la Serie Temporal\n",
    "\n",
    "1. ¿Cuál es el comportamiento general de las ventas a lo largo del tiempo?\n",
    "   - ¿Existen patrones claros en la serie temporal?\n",
    "   - ¿Hay valores atípicos o cambios bruscos significativos?\n",
    "   - ¿Se pueden identificar componentes de estacionalidad o tendencia a simple vista?\n",
    "\n",
    "2. ¿Cómo varían las tendencias según la escala temporal analizada?\n",
    "   - ¿Qué patrones emergen al analizar ventanas mensuales (30 días)?\n",
    "   - ¿Existen ciclos trimestrales (90 días) identificables?\n",
    "   - ¿Cuál es la tendencia general anual (365 días) de las ventas?\n",
    "   - ¿Cómo se relacionan los patrones entre diferentes escalas temporales?\n",
    "\n",
    "3. ¿Cuáles son los componentes fundamentales que explican el comportamiento de las ventas?\n",
    "   - ¿Qué proporción de la variabilidad se explica por la tendencia?\n",
    "   - ¿Qué tan fuerte es el componente estacional?\n",
    "   - ¿Existe un patrón sistemático en los residuos?\n",
    "   - ¿Hay eventos o factores externos que afecten significativamente la serie?\n",
    "\n",
    "4. ¿Cómo podemos utilizar estos insights para mejorar nuestro modelo?\n",
    "   - ¿Qué características deberíamos incluir en el modelo?\n",
    "   - ¿Qué transformaciones de datos serían más apropiadas?\n",
    "   - ¿Qué métricas base podemos establecer para evaluar el rendimiento?\n",
    "\n",
    "   ---\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptos Clave y Técnicas: Visualización Series Temporales\n",
    "\n",
    "\n",
    "**Series Temporales**\n",
    "- Secuencia de datos ordenados cronológicamente que captura la evolución de las ventas\n",
    "- Permite identificar patrones, ciclos y comportamientos a lo largo del tiempo\n",
    "- Base fundamental para el análisis predictivo de las ventas futuras\n",
    "\n",
    "**Media Móvil**\n",
    "- Técnica de suavizado que promedia valores en una ventana deslizante\n",
    "- Reduce el ruido y resalta tendencias subyacentes\n",
    "- Ventanas utilizadas:\n",
    "  * 30 días: patrones mensuales\n",
    "  * 90 días: patrones trimestrales\n",
    "  * 365 días: tendencia anual\n",
    "\n",
    "**Descomposición Estacional**\n",
    "- Separa la serie temporal en componentes fundamentales:\n",
    "  * Tendencia: patrón de largo plazo que indica la dirección general\n",
    "  * Estacionalidad: patrones cíclicos que se repiten en intervalos fijos\n",
    "  * Residuos: variaciones aleatorias no explicadas por tendencia ni estacionalidad\n",
    "\n",
    "**Análisis de Ruido y Residuos**\n",
    "- Ruido: fluctuaciones aleatorias de alta frecuencia en los datos\n",
    "- Residuos: diferencia entre valores observados y componentes sistemáticos\n",
    "- Importancia para:\n",
    "  * Evaluar la calidad del modelo\n",
    "  * Identificar anomalías\n",
    "  * Verificar supuestos estadísticos\n",
    "\n",
    "**Patrones y Ciclos**\n",
    "- Estacionalidad: variaciones regulares y predecibles\n",
    "- Ciclos: fluctuaciones no necesariamente regulares\n",
    "- Tendencias: dirección general del comportamiento a largo plazo\n",
    "\n",
    "**Valores Atípicos**\n",
    "- Observaciones que se desvían significativamente del patrón esperado\n",
    "- Pueden indicar:\n",
    "  * Eventos especiales\n",
    "  * Errores en los datos\n",
    "  * Cambios estructurales en el negocio\n",
    "\n",
    "Estas técnicas y conceptos nos permitirán:\n",
    "- Comprender la estructura temporal de las ventas\n",
    "- Identificar factores que influyen en el comportamiento\n",
    "- Preparar los datos para el modelado predictivo\n",
    "- Establecer bases para la evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Análisis Estático\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histograma de distribución de ventas\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Cantidad_Vendida'].hist(bins=50, edgecolor='black')\n",
    "plt.title('Distribución de Ventas Diarias')\n",
    "plt.xlabel('Cantidad Vendida')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Frecuencia de ventas diarias promedio en un año\n",
    "df['DiaDelAño'] = df.index.dayofyear\n",
    "promedio_anual = df.groupby('DiaDelAño')['Cantidad_Vendida'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "promedio_anual.hist(bins=50, edgecolor='black')\n",
    "plt.title('Distribución de Ventas Diarias (Promedio Anual)')\n",
    "plt.xlabel('Cantidad Vendida Promedio')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Frecuencia de ventas diarias promedio en un mes\n",
    "df['DiaDelMes'] = df.index.day\n",
    "promedio_mensual = df.groupby('DiaDelMes')['Cantidad_Vendida'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "promedio_mensual.hist(bins=15, edgecolor='black')\n",
    "plt.title('Distribución de Ventas Diarias (Promedio Mensual)')\n",
    "plt.xlabel('Cantidad Vendida Promedio')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mensual:\n",
    "\n",
    "\t•\tEl patrón sugiere que hay bastante variabilidad estacional dentro del año.\n",
    "\n",
    "Diario:\t\n",
    "    \n",
    "    •\tLa dispersión es más limitada que en el gráfico anual, lo cual es lógico porque se eliminan las variaciones estacionales más largas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KDE Plot (distribución suavizada)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(df['Cantidad_Vendida'], shade=True)\n",
    "plt.title('Densidad Suavizada de Ventas Diarias')\n",
    "plt.xlabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot global\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.boxplot(y=df['Cantidad_Vendida'])\n",
    "plt.title('Boxplot Global de Ventas Diarias')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QQ Plot (normalidad: desviación de la normalidad es desviación de la diagonal)\n",
    "sm.qqplot(df['Cantidad_Vendida'], line='s')\n",
    "plt.title('QQ Plot de Ventas Diarias')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot por día de la semana\n",
    "df['DiaSemana'] = df.index.dayofweek\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='DiaSemana', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Boxplot por Día de la Semana (0=Lunes, 6=Domingo)')\n",
    "plt.xlabel('Día de la Semana')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Boxplot por mes\n",
    "df['Mes'] = df.index.month\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='Mes', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Boxplot por Mes ')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Promedio por mes del año\n",
    "df['Mes'] = df.index.month\n",
    "prom_mensual = df.groupby('Mes')['Cantidad_Vendida'].mean()\n",
    "plt.figure(figsize=(10, 5))\n",
    "prom_mensual.plot(kind='bar')\n",
    "plt.title('Promedio de Ventas por Mes del Año')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Cantidad Promedio Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Promedio por semana del año\n",
    "df['Semana'] = df.index.isocalendar().week\n",
    "prom_semanal = df.groupby('Semana')['Cantidad_Vendida'].mean()\n",
    "plt.figure(figsize=(12, 5))\n",
    "prom_semanal.plot()\n",
    "plt.title('Promedio de Ventas por Semana del Año')\n",
    "plt.xlabel('Semana')\n",
    "plt.ylabel('Cantidad Promedio Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparar columnas derivadas\n",
    "df['DiaDelAnio'] = df.index.dayofyear\n",
    "df['DiaDelMes'] = df.index.day\n",
    "df['DiaSemana'] = df.index.dayofweek\n",
    "df['Mes'] = df.index.month\n",
    "df['Anio'] = df.index.year\n",
    "\n",
    "\n",
    "# Serie completa\n",
    "sns.kdeplot(df['Cantidad_Vendida'], label='Global', linewidth=2)\n",
    "\n",
    "# Promedio por día del año (acumulado entre años)\n",
    "prom_dia_anio = df.groupby('DiaDelAnio')['Cantidad_Vendida'].mean()\n",
    "sns.kdeplot(prom_dia_anio, label='Promedio por Día del Año')\n",
    "\n",
    "# Promedio por mes\n",
    "prom_mes = df.groupby('Mes')['Cantidad_Vendida'].mean()\n",
    "sns.kdeplot(prom_mes, label='Promedio por Mes')\n",
    "\n",
    "# Promedio por día del mes\n",
    "prom_dia_mes = df.groupby('DiaDelMes')['Cantidad_Vendida'].mean()\n",
    "sns.kdeplot(prom_dia_mes, label='Promedio por Día del Mes')\n",
    "\n",
    "# Promedio por día de la semana\n",
    "prom_semana = df.groupby('DiaSemana')['Cantidad_Vendida'].mean()\n",
    "sns.kdeplot(prom_semana, label='Promedio por Día de Semana')\n",
    "\n",
    "plt.title('KDE Comparativo: Niveles Agregados sin Tiempo Explícito')\n",
    "plt.xlabel('Cantidad Vendida Promedio')\n",
    "plt.ylabel('Densidad')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########################∫\n",
    "# Mismo gráfico sin el promedio por día del año\n",
    "plt.figure()\n",
    "sns.kdeplot(df['Cantidad_Vendida'], label='Global', linewidth=2)\n",
    "\n",
    "# Promedio por mes\n",
    "sns.kdeplot(prom_mes, label='Promedio por Mes')\n",
    "\n",
    "# Promedio por día del año\n",
    "sns.kdeplot(prom_dia_anio, label='Promedio por Día del Año')\n",
    "\n",
    "# Promedio por día de la semana\n",
    "sns.kdeplot(prom_semana, label='Promedio por Día de Semana')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Clasificación por quintil\n",
    "df['Quintil'] = pd.qcut(df['Cantidad_Vendida'], 5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Violin plot por quintil\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(x='Quintil', y='Cantidad_Vendida', data=df, order=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "plt.title('Distribución de Ventas por Quintil (Violin Plot)')\n",
    "plt.xlabel('Quintil')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# KDE por quintil\n",
    "plt.figure(figsize=(10, 5))\n",
    "for quintil in ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']:\n",
    "    sns.kdeplot(df[df['Quintil'] == quintil]['Cantidad_Vendida'], label=quintil)\n",
    "\n",
    "plt.title('Distribución KDE por Quintil de Ventas')\n",
    "plt.xlabel('Cantidad Vendida')\n",
    "plt.ylabel('Densidad')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vuelta 3!!! - Datos de series de tiempo\n",
    "\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Análisis Dinámico \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la serie de tiempo completa\n",
    "plt.figure(figsize=(16, 6))\n",
    "df['Cantidad_Vendida'].plot()\n",
    "plt.title('Ventas Diarias de Croissants de Almendras (5 Años)')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.xlabel('Fecha')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Media móvil de 7 días\n",
    "df['MediaMovil7'] = df['Cantidad_Vendida'].rolling(window=7).mean()\n",
    "plt.figure(figsize=(12, 5))\n",
    "df['Cantidad_Vendida'].plot(alpha=0.3, label='Original')\n",
    "df['MediaMovil7'].plot(label='Media Móvil (7 días)', linewidth=2)\n",
    "plt.title('Media Móvil de Ventas Diarias (7 días)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Media móvil de 7 días\n",
    "df['MediaMovil30'] = df['Cantidad_Vendida'].rolling(window=30).mean()\n",
    "plt.figure(figsize=(12, 5))\n",
    "df['Cantidad_Vendida'].plot(alpha=0.3, label='Original')\n",
    "df['MediaMovil30'].plot(label='Media Móvil (30 días)', linewidth=2)\n",
    "plt.title('Media Móvil de Ventas Diarias (30 días)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scatter: ventas vs día del año\n",
    "df['DiaDelAño'] = df.index.dayofyear\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['DiaDelAño'], df['Cantidad_Vendida'], alpha=0.3)\n",
    "plt.title('Ventas vs Día del Año')\n",
    "plt.xlabel('Día del Año')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter: ventas vs día del mes\n",
    "df['DiaDelMes'] = df.index.day\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['DiaDelMes'], df['Cantidad_Vendida'], alpha=0.3)\n",
    "plt.title('Ventas vs Día del Mes')\n",
    "plt.xlabel('Día del Mes')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boxplot por año\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['Año'] = df.index.year\n",
    "df.boxplot(column='Cantidad_Vendida', by='Año')\n",
    "plt.title('Distribución Anual de Ventas')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Promedio mensual (para ver estacionalidad en agregados)\n",
    "df['Mes'] = df.index.month\n",
    "promedios_mensuales = df.groupby('Mes')['Cantidad_Vendida'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "promedios_mensuales.plot(marker='o')\n",
    "plt.title('Promedio de Ventas por Mes (Agregado Anual)')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Cantidad Promedio Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(range(1, 13))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Día del mes\n",
    "df['DiaDelMes'] = df.index.day\n",
    "\n",
    "# Promedio por día del mes (agregado en todos los meses)\n",
    "promedio_diario_mes = df.groupby('DiaDelMes')['Cantidad_Vendida'].mean()\n",
    "\n",
    "# Gráfico del promedio de ventas por día del mes\n",
    "plt.figure(figsize=(12, 5))\n",
    "promedio_diario_mes.plot(marker='o')\n",
    "plt.title('Promedio de Ventas por Día del Mes (Agregado Anual)')\n",
    "plt.xlabel('Día del Mes')\n",
    "plt.ylabel('Cantidad Promedio Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(range(1, 32))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular y graficar media móvil para suavizar y ver tendencias/ciclos\n",
    "plt.figure(figsize=(16, 6))\n",
    "df['Cantidad_Vendida'].plot(label='Diario', alpha=0.6, style='.') # Usar puntos para ver mejor densidad\n",
    "df['Cantidad_Vendida'].rolling(window=30).mean().plot(label='Media Móvil 30 días', linewidth=2)\n",
    "df['Cantidad_Vendida'].rolling(window=90).mean().plot(label='Media Móvil 90 días', linewidth=2, linestyle='--')\n",
    "df['Cantidad_Vendida'].rolling(window=365).mean().plot(label='Media Móvil 365 días', linewidth=3, color='red')\n",
    "plt.title('Ventas Diarias y Medias Móviles')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.xlabel('Fecha')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse que la serie tenga frecuencia diaria\n",
    "ts_data = df['Cantidad_Vendida'].asfreq('D') # Rellenará con NaN si faltan fechas\n",
    "\n",
    "# La frecuencia diaria es necesaria para la descomposición estacional porque:\n",
    "# 1. Implica el mayor nivel de detalle posible. Son los saltos \"más chicos\" en la serie. \n",
    "# 1a. Asegura que no haya \"huecos\" en la serie temporal que distorsionen el análisis\n",
    "# 2. Permite identificar correctamente los patrones estacionales al tener observaciones equidistantes\n",
    "# 3. Es requerido por el método seasonal_decompose que espera datos con frecuencia regular\n",
    "\n",
    "ts_data.fillna(method='ffill', inplace=True) # Rellenar NaNs si los hubiera\n",
    "\n",
    "\t# •\tQuiero descomponer una serie de tiempo (ts_data)\n",
    "\t# •\tBajo la suposición de que los componentes se suman entre sí (model='additive')\n",
    "\t# •\tY que el patrón estacional se repite cada 365 días (period=365), es decir, un ciclo anual\n",
    "\n",
    "\n",
    "# El period=365 indica que buscamos patrones anuales, pero podríamos usar:\n",
    "# - period=7 para patrones semanales\n",
    "# - period=30/31 para patrones mensuales\n",
    "# - otros valores según el ciclo estacional que queramos analizar\n",
    "\n",
    "# sm.tsa.seasonal_decompose() descompone una serie temporal en 3 componentes:\n",
    "# - Tendencia: dirección a largo plazo de la serie\n",
    "# - Estacionalidad: patrones cíclicos que se repiten con frecuencia fija\n",
    "# - Residuos: variaciones aleatorias no explicadas por tendencia ni estacionalidad\n",
    "# model='additive' asume que los componentes se suman (vs multiplicativo donde se multiplican)\n",
    "\n",
    "\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(ts_data, model='additive', period=365)\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(14, 10)\n",
    "plt.suptitle('Descomposición Estacional Aditiva (Ciclo Anual)', y=1.01)\n",
    "# Rotar etiquetas del eje x para mejor legibilidad\n",
    "for ax in fig.axes:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El mismo gráfico de arriba, pero con los componentes separados\n",
    "\n",
    "# Gráfico: Tendencia\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(decomposition.trend)\n",
    "plt.title('Componente: Tendencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico: Estacionalidad\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.title('Componente: Estacionalidad')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico: Residuos\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(decomposition.resid)\n",
    "plt.title('Componente: Residuos')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación EDA - Variable Objetivo:**\n",
    "*   **Tendencia:** ¿La media móvil de 365 días muestra una tendencia general (creciente, decreciente, estable)? (Esperamos un ligero crecimiento por la simulación).\n",
    "*   **Estacionalidad Anual:** ¿Se observa claramente el ciclo en la media móvil de 30/90 días y en la componente `seasonal` de la descomposición? (Ventas más altas en invierno, más bajas en verano).\n",
    "*   **Eventos:** ¿Hay picos o valles abruptos en la serie diaria? ¿Coinciden visualmente con feriados o posibles promociones?\n",
    "*   **Ruido/Residuos:** La componente `resid` muestra la variabilidad no explicada por la tendencia y la estacionalidad simple. ¿Parece aleatoria o tiene algún patrón?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_4.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación: categóricas y binarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Influencia de Variables Categóricas / Binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear mapeo de días a números\n",
    "dia_a_numero = {'lunes': 0, 'martes': 1, 'miércoles': 2, 'jueves': 3, 'viernes': 4, 'sábado': 5, 'domingo': 6}\n",
    "df['Dia_Semana_Num'] = df['Dia_Semana'].map(dia_a_numero)\n",
    "\n",
    "# Crear el gráfico\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Distribución de Ventas por Categorías', fontsize=16)\n",
    "\n",
    "# Ordenar días de la semana correctamente\n",
    "weekday_order = list(range(7))  # 0 a 6\n",
    "sns.boxplot(ax=axes[0, 0], data=df, x='Dia_Semana_Num', y='Cantidad_Vendida', order=weekday_order)\n",
    "axes[0, 0].set_title('Ventas por Día de la Semana')\n",
    "axes[0, 0].set_xticklabels(['Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo'], rotation=30)\n",
    "axes[0, 0].set_xlabel('')\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='Mes', y='Cantidad_Vendida')\n",
    "axes[0, 1].set_title('Ventas por Mes')\n",
    "axes[0, 1].set_xlabel('Mes del Año')\n",
    "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "sns.boxplot(ax=axes[1, 0], data=df, x='Es_Feriado', y='Cantidad_Vendida')\n",
    "axes[1, 0].set_title('Ventas en Días Feriados/Puente vs. Normales')\n",
    "axes[1, 0].set_xticklabels(['Normal (0)', 'Feriado/Puente (1)'])\n",
    "axes[1, 0].set_xlabel('')\n",
    "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "sns.boxplot(ax=axes[1, 1], data=df, x='Promocion_Croissant', y='Cantidad_Vendida')\n",
    "axes[1, 1].set_title('Ventas con Promoción vs. Sin Promoción')\n",
    "axes[1, 1].set_xticklabels(['Sin Promoción (0)', 'Con Promoción (1)'])\n",
    "axes[1, 1].set_xlabel('')\n",
    "axes[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Ajustar para el titulo gral\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear indicador de fin de semana\n",
    "df['Es_Fin_Semana'] = df['Dia_Semana'].isin(['sábado', 'domingo']).astype(int)\n",
    "\n",
    "# Frecuencias relativas generales - subgráficos\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Obtener colores del estilo actual\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Primer subgráfico - Feriados\n",
    "df['Es_Feriado'].value_counts(normalize=True).plot(kind='bar', ax=ax1, color=[colors[0], colors[1]])\n",
    "ax1.set_title('Frecuencia Relativa:\\nDías Normales vs Feriados (Total)')\n",
    "ax1.set_xlabel('Tipo de Día')\n",
    "ax1.set_ylabel('Frecuencia Relativa')\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.set_xticklabels(['Días Normales', 'Días Feriados'], rotation=0)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Segundo subgráfico - Promociones\n",
    "df['Promocion_Croissant'].value_counts(normalize=True).plot(kind='bar', ax=ax2, color=[colors[0], colors[1]])\n",
    "ax2.set_title('Frecuencia Relativa:\\nSin vs Con Promoción (Total)')\n",
    "ax2.set_xlabel('Promoción')\n",
    "ax2.set_ylabel('Frecuencia Relativa')\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax2.set_xticklabels(['Sin Promoción', 'Con Promoción'], rotation=0)\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Tercer subgráfico - Fines de semana\n",
    "df['Es_Fin_Semana'].value_counts(normalize=True).plot(kind='bar', ax=ax3, color=[colors[0], colors[1]])\n",
    "ax3.set_title('Frecuencia Relativa:\\nDías Laborales vs Fin de Semana (Total)')\n",
    "ax3.set_xlabel('Tipo de Día')\n",
    "ax3.set_ylabel('Frecuencia Relativa')\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax3.set_xticklabels(['Días Laborales', 'Fin de Semana'], rotation=0)\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Por año\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby(['Anio', 'Es_Feriado']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Días Normales vs Feriados por Año')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Días Normales', 'Días Feriados'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby(['Anio', 'Promocion_Croissant']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Sin vs Con Promoción por Año')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Sin Promoción', 'Con Promoción'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby(['Anio', 'Es_Fin_Semana']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Días Laborales vs Fin de Semana por Año')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Días Laborales', 'Fin de Semana'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Por mes\n",
    "plt.figure(figsize=(14, 6))\n",
    "df.groupby(['Mes', 'Es_Feriado']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Días Normales vs Feriados por Mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Días Normales', 'Días Feriados'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "df.groupby(['Mes', 'Promocion_Croissant']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Sin vs Con Promoción por Mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Sin Promoción', 'Con Promoción'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "df.groupby(['Mes', 'Es_Fin_Semana']).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar')\n",
    "plt.title('Frecuencia Relativa: Días Laborales vs Fin de Semana por Mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Frecuencia Relativa')\n",
    "plt.legend(['Días Laborales', 'Fin de Semana'], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear combinaciones de categorías\n",
    "df['Categoria'] = 'Día Normal'  # categoría por defecto\n",
    "\n",
    "# Actualizar categorías basadas en combinaciones\n",
    "df.loc[df['Es_Fin_Semana'] == 1, 'Categoria'] = 'Fin de Semana'\n",
    "df.loc[df['Es_Feriado'] == 1, 'Categoria'] = 'Feriado'\n",
    "df.loc[df['Promocion_Croissant'] == 1, 'Categoria'] = 'Promoción'\n",
    "\n",
    "# Crear combinaciones detalladas\n",
    "df['Categoria_Detallada'] = 'Día Normal'\n",
    "df.loc[df['Es_Fin_Semana'] == 1, 'Categoria_Detallada'] = 'Fin de Semana'\n",
    "df.loc[df['Es_Feriado'] == 1, 'Categoria_Detallada'] = 'Feriado'\n",
    "df.loc[(df['Es_Fin_Semana'] == 1) & (df['Es_Feriado'] == 1), 'Categoria_Detallada'] = 'Feriado en Fin de Semana'\n",
    "df.loc[df['Promocion_Croissant'] == 1, 'Categoria_Detallada'] = 'Promoción'\n",
    "df.loc[(df['Promocion_Croissant'] == 1) & (df['Es_Fin_Semana'] == 1), 'Categoria_Detallada'] = 'Promoción en Fin de Semana'\n",
    "df.loc[(df['Promocion_Croissant'] == 1) & (df['Es_Feriado'] == 1), 'Categoria_Detallada'] = 'Promoción en Feriado'\n",
    "df.loc[(df['Promocion_Croissant'] == 1) & (df['Es_Feriado'] == 1) & (df['Es_Fin_Semana'] == 1), 'Categoria_Detallada'] = 'Promoción en Feriado y Fin de Semana'\n",
    "\n",
    "# Gráfico 1: Distribución general\n",
    "plt.figure(figsize=(15, 6))\n",
    "cat_counts = df['Categoria_Detallada'].value_counts()\n",
    "cat_freq = (cat_counts / len(df) * 100).round(2)\n",
    "bars = plt.bar(range(len(cat_freq)), cat_freq)\n",
    "plt.title('Distribución de Días por Categoría (%)')\n",
    "plt.xlabel('Categoría')\n",
    "plt.ylabel('Porcentaje')\n",
    "plt.xticks(range(len(cat_freq)), cat_freq.index, rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Añadir etiquetas de porcentaje sobre las barras\n",
    "for i, v in enumerate(cat_freq):\n",
    "    plt.text(i, v + 0.5, f'{v:.1f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 2: Distribución de ventas por categoría\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=df, x='Categoria_Detallada', y='Cantidad_Vendida')\n",
    "plt.title('Distribución de Ventas por Categoría')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 3: Ventas promedio por categoría\n",
    "plt.figure(figsize=(15, 6))\n",
    "mean_sales = df.groupby('Categoria_Detallada')['Cantidad_Vendida'].mean().sort_values(ascending=False)\n",
    "bars = plt.bar(range(len(mean_sales)), mean_sales)\n",
    "plt.title('Promedio de Ventas por Categoría')\n",
    "plt.xlabel('Categoría')\n",
    "plt.ylabel('Promedio de Ventas')\n",
    "plt.xticks(range(len(mean_sales)), mean_sales.index, rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Añadir valores promedio sobre las barras\n",
    "for i, v in enumerate(mean_sales):\n",
    "    plt.text(i, v + 0.5, f'{v:.1f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 4: Gráfico KDE comparando distribuciones\n",
    "plt.figure(figsize=(15, 6))\n",
    "for categoria in df['Categoria_Detallada'].unique():\n",
    "    sns.kdeplot(data=df[df['Categoria_Detallada'] == categoria]['Cantidad_Vendida'], \n",
    "                label=categoria)\n",
    "plt.title('Distribución de Ventas por Categoría (KDE)')\n",
    "plt.xlabel('Cantidad Vendida')\n",
    "plt.ylabel('Densidad')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 5: Patrones mensuales\n",
    "plt.figure(figsize=(15, 6))\n",
    "monthly_avg = df.groupby(['Mes', 'Categoria_Detallada'])['Cantidad_Vendida'].mean().unstack()\n",
    "monthly_avg.plot(marker='o')\n",
    "plt.title('Promedio de Ventas por Categoría - Evolución Mensual')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Promedio de Ventas')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dos visualizaciones para analizar patrones en las ventas según diferentes categorías de días\n",
    "\n",
    "# Primera visualización: Gráfico de dispersión que muestra los datos individuales\n",
    "# Creamos una figura de tamaño grande para mejor visualización\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Graficamos los días normales (sin feriados, sin fin de semana, sin promociones)\n",
    "# Usamos puntos pequeños y transparentes para evitar saturación visual\n",
    "normal_days = df[(df['Es_Feriado'] == 0) & (df['Es_Fin_Semana'] == 0) & (df['Promocion_Croissant'] == 0)]\n",
    "plt.scatter(normal_days.index.dayofyear, normal_days['Cantidad_Vendida'], \n",
    "    alpha=0.3, color='gray', label='Días Normales', s=30)\n",
    "\n",
    "# Graficamos los feriados con puntos rojos más grandes\n",
    "holidays = df[df['Es_Feriado'] == 1]\n",
    "plt.scatter(holidays.index.dayofyear, holidays['Cantidad_Vendida'], \n",
    "    alpha=0.6, color='red', label='Feriados', s=50)\n",
    "\n",
    "# Graficamos los fines de semana sin promoción en azul\n",
    "weekends = df[(df['Es_Fin_Semana'] == 1) & (df['Promocion_Croissant'] == 0) & (df['Es_Feriado'] == 0)]\n",
    "plt.scatter(weekends.index.dayofyear, weekends['Cantidad_Vendida'], \n",
    "    alpha=0.6, color='blue', label='Fines de Semana', s=50)\n",
    "\n",
    "# Graficamos los días con promoción (que no son fin de semana) en verde\n",
    "promos = df[(df['Promocion_Croissant'] == 1) & (df['Es_Fin_Semana'] == 0) & (df['Es_Feriado'] == 0)]\n",
    "plt.scatter(promos.index.dayofyear, promos['Cantidad_Vendida'], \n",
    "    alpha=0.6, color='green', label='Promociones', s=50)\n",
    "\n",
    "# Graficamos los fines de semana con promoción en morado\n",
    "weekend_promos = df[(df['Es_Fin_Semana'] == 1) & (df['Promocion_Croissant'] == 1)]\n",
    "plt.scatter(weekend_promos.index.dayofyear, weekend_promos['Cantidad_Vendida'], \n",
    "    alpha=0.6, color='purple', label='Fines de Semana + Promociones', s=50)\n",
    "\n",
    "# Configuramos los elementos del gráfico\n",
    "plt.title('Ventas por Día del Año según Categoría')\n",
    "plt.xlabel('Día del Año')\n",
    "plt.ylabel('Cantidad Vendida')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Segunda visualización: Media móvil para ver tendencias más claras\n",
    "# Usamos una ventana de 30 días para suavizar las fluctuaciones diarias\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Definimos el tamaño de la ventana para la media móvil\n",
    "window = 30  # ventana de 30 días\n",
    "\n",
    "# Calculamos y graficamos la media móvil para cada categoría\n",
    "# Días normales\n",
    "normal_rolling = normal_days['Cantidad_Vendida'].rolling(window=window, min_periods=1).mean()\n",
    "plt.plot(normal_days.index.dayofyear, normal_rolling, \n",
    "    color='gray', label='Días Normales', alpha=0.8)\n",
    "\n",
    "# Feriados\n",
    "holiday_rolling = holidays['Cantidad_Vendida'].rolling(window=window, min_periods=1).mean()\n",
    "plt.plot(holidays.index.dayofyear, holiday_rolling, \n",
    "    color='red', label='Feriados', alpha=0.8)\n",
    "\n",
    "# Fines de semana\n",
    "weekend_rolling = weekends['Cantidad_Vendida'].rolling(window=window, min_periods=1).mean()\n",
    "plt.plot(weekends.index.dayofyear, weekend_rolling, \n",
    "    color='blue', label='Fines de Semana', alpha=0.8)\n",
    "\n",
    "# Días con promoción\n",
    "promo_rolling = promos['Cantidad_Vendida'].rolling(window=window, min_periods=1).mean()\n",
    "plt.plot(promos.index.dayofyear, promo_rolling, \n",
    "    color='green', label='Promociones', alpha=0.8)\n",
    "\n",
    "# Fines de semana con promoción\n",
    "weekend_promo_rolling = weekend_promos['Cantidad_Vendida'].rolling(window=window, min_periods=1).mean()\n",
    "plt.plot(weekend_promos.index.dayofyear, weekend_promo_rolling, \n",
    "    color='purple', label='Fines de Semana + Promociones', alpha=0.8)\n",
    "\n",
    "# Configuramos los elementos del gráfico de medias móviles\n",
    "plt.title(f'Media Móvil ({window} días) de Ventas por Categoría')\n",
    "plt.xlabel('Día del Año')\n",
    "plt.ylabel('Cantidad Vendida (Media Móvil)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Análisis de tendencia temporal\n",
    "plt.figure(figsize=(15, 6))\n",
    "df['Fecha'] = pd.to_datetime(df[['Anio', 'Mes', 'DiaDelMes']].rename(\n",
    "    columns={'Anio': 'year', 'Mes': 'month', 'DiaDelMes': 'day'}))\n",
    "df.set_index('Fecha', inplace=True)\n",
    "\n",
    "# Gráfico de serie temporal con media móvil\n",
    "plt.plot(df['Cantidad_Vendida'], alpha=0.3, label='Ventas Diarias')\n",
    "plt.plot(df['MediaMovil7'], label='Media Móvil 7 días', linewidth=2)\n",
    "plt.title('Evolución de Ventas con Media Móvil')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2. Comparación de ventas por categoría\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Categoria', y='Cantidad_Vendida', data=df,\n",
    "           order=['Día Normal', 'Fin de Semana', 'Feriado', 'Promoción'])\n",
    "plt.title('Distribución de Ventas por Categoría')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 3. Análisis de ventas por quintil\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Quintil', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Distribución de Ventas por Quintil')\n",
    "plt.show()\n",
    "\n",
    "# 4. Comparación de ventas en días normales vs. especiales\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Es_Fin_Semana', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Comparación de Ventas: Días de Semana vs. Fin de Semana')\n",
    "plt.xticks([0, 1], ['Días de Semana', 'Fin de Semana'])\n",
    "plt.show()\n",
    "\n",
    "# 5. Análisis de ventas por mes y año\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(x='Mes', y='Cantidad_Vendida', hue='Anio', data=df)\n",
    "plt.title('Distribución de Ventas por Mes y Año')\n",
    "plt.legend(title='Año')\n",
    "plt.show()\n",
    "\n",
    "# 6. Análisis de ventas acumuladas\n",
    "plt.figure(figsize=(15, 6))\n",
    "df['Acumulado_Anual'] = df.groupby('Anio')['Cantidad_Vendida'].cumsum()\n",
    "sns.lineplot(x='Dia_Anio', y='Acumulado_Anual', hue='Anio', data=df)\n",
    "plt.title('Ventas Acumuladas por Año')\n",
    "plt.xlabel('Día del Año')\n",
    "plt.ylabel('Ventas Acumuladas')\n",
    "plt.show()\n",
    "\n",
    "# 7. Comparación de ventas en días con/sin promoción\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Promocion_Croissant', y='Cantidad_Vendida', \n",
    "           hue='Es_Fin_Semana', data=df)\n",
    "plt.title('Efecto de Promociones en Días de Semana y Fin de Semana')\n",
    "plt.xticks([0, 1], ['Sin Promoción', 'Con Promoción'])\n",
    "plt.legend(title='Fin de Semana', labels=['No', 'Sí'])\n",
    "plt.show()\n",
    "\n",
    "# 8. Análisis de ventas por categoría detallada\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Categoria_Detallada', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Distribución de Ventas por Categoría Detallada')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vuelta 4!!, de nuevo exploramos\n",
    "\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_4.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_4.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas vs Temperatura Máxima\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.scatterplot(data=df, x='Temperatura_Max_Prevista', y='Cantidad_Vendida', alpha=0.2, s=100, color='blue')\n",
    "plt.title('Relación con el entorno - Ventas vs. Temperatura Máx.')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ventas vs Precio Nuestro\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.scatterplot(data=df, x='Precio_Nuestro', y='Cantidad_Vendida', alpha=0.2, s=100, color='green')\n",
    "plt.title('Relación con el entorno - Ventas vs. Precio Nuestro')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ventas vs Precio Competencia\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.scatterplot(data=df, x='Precio_Competencia', y='Cantidad_Vendida', alpha=0.2, s=100, color='purple')\n",
    "plt.title('Relación con el entorno - Ventas vs. Precio Competencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Variables categóricas con tratamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA - Variables Categóricas clave:**\n",
    "*   **Día Semana:** ¿Se confirma el aumento de ventas (mediana y dispersión) en fines de semana (Sábado/Domingo)?\n",
    "*   **Mes:** ¿El patrón estacional visto antes se confirma aquí? (Meses de invierno con medianas más altas).\n",
    "*   **Feriado:** ¿Hay una diferencia clara y positiva en las ventas durante feriados/puentes?\n",
    "*   **Promoción:** ¿Las promociones tienen un efecto positivo y significativo en las ventas?\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### aquí la diferencia no son los gráficos, sino el análisis y los indicadores estadísticos que vamos a calcular. \n",
    "\n",
    "# 1. Análisis por Día de la Semana\n",
    "print(\"=== ANÁLISIS POR DÍA DE LA SEMANA ===\")\n",
    "ventas_dia = df.groupby('Dia_Semana')['Cantidad_Vendida'].agg(['median', 'mean', 'std', 'count'])\n",
    "print(ventas_dia)\n",
    "\n",
    "# 2. Análisis por Mes\n",
    "print(\"\\n=== ANÁLISIS POR MES ===\")\n",
    "ventas_mes = df.groupby('Mes')['Cantidad_Vendida'].agg(['median', 'mean', 'std', 'count'])\n",
    "print(ventas_mes)\n",
    "\n",
    "# 3. Análisis por Feriado\n",
    "print(\"\\n=== ANÁLISIS POR FERIADO ===\")\n",
    "ventas_feriado = df.groupby('Es_Feriado')['Cantidad_Vendida'].agg(['median', 'mean', 'std', 'count'])\n",
    "print(ventas_feriado)\n",
    "\n",
    "# 4. Análisis por Promoción\n",
    "print(\"\\n=== ANÁLISIS POR PROMOCIÓN ===\")\n",
    "ventas_promocion = df.groupby('Promocion_Croissant')['Cantidad_Vendida'].agg(['median', 'mean', 'std', 'count'])\n",
    "print(ventas_promocion)\n",
    "\n",
    "# 5. Análisis de significancia estadística\n",
    "\n",
    "# Prueba t para feriados\n",
    "feriado_si = df[df['Es_Feriado'] == 1]['Cantidad_Vendida']\n",
    "feriado_no = df[df['Es_Feriado'] == 0]['Cantidad_Vendida']\n",
    "t_stat, p_val = stats.ttest_ind(feriado_si, feriado_no, equal_var=False)\n",
    "print(f\"\\nPrueba t para feriados: t = {t_stat:.2f}, p-valor = {p_val:.4f}\")\n",
    "\n",
    "# Prueba t para promociones\n",
    "promo_si = df[df['Promocion_Croissant'] == 1]['Cantidad_Vendida']\n",
    "promo_no = df[df['Promocion_Croissant'] == 0]['Cantidad_Vendida']\n",
    "t_stat, p_val = stats.ttest_ind(promo_si, promo_no, equal_var=False)\n",
    "print(f\"Prueba t para promociones: t = {t_stat:.2f}, p-valor = {p_val:.4f}\")\n",
    "\n",
    "# Análisis de varianza (ANOVA) para días de la semana\n",
    "f_stat, p_val = stats.f_oneway(*[df[df['Dia_Semana'] == dia]['Cantidad_Vendida'] for dia in df['Dia_Semana'].unique()])\n",
    "print(f\"ANOVA para días de la semana: F = {f_stat:.2f}, p-valor = {p_val:.4f}\")\n",
    "\n",
    "# Análisis de varianza (ANOVA) para meses\n",
    "f_stat, p_val = stats.f_oneway(*[df[df['Mes'] == mes]['Cantidad_Vendida'] for mes in df['Mes'].unique()])\n",
    "print(f\"ANOVA para meses: F = {f_stat:.2f}, p-valor = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Modelización exploratoria\n",
    "\n",
    "--- \n",
    "\n",
    "## Anova \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a ejecutar un ANOVA para Días de la Semana\n",
    "\n",
    "#### ¿Qué es el ANOVA?\n",
    "\n",
    "ANOVA (Análisis de Varianza) es una herramienta estadística que nos ayuda a responder la pregunta: \n",
    "**\"¿Las ventas son diferentes según el día de la semana?\"**\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "\n",
    "1. **Compara dos tipos de variación**:\n",
    "   - **Variación entre grupos**: ¿Cuánto difieren las ventas entre lunes, martes, miércoles, etc.?\n",
    "   - **Variación dentro de grupos**: ¿Cuánto varían las ventas dentro de un mismo día?\n",
    "\n",
    "2. **El valor F (111.56)** nos dice:\n",
    "   - Si F es grande (como en este caso), significa que las diferencias entre días son mucho más importantes que las variaciones dentro de cada día\n",
    "\n",
    "\n",
    "# 3. La fórmula matemática del ANOVA:\n",
    "\n",
    "F = (Varianza entre grupos) / (Varianza dentro de grupos)\n",
    "\n",
    "F = (MSB) / (MSW)\n",
    "    \n",
    "Donde:\n",
    "- MSB (Mean Square Between) = SSB / (k-1)\n",
    "- MSW (Mean Square Within) = SSW / (N-k)\n",
    "- SSB = Σ nᵢ(x̄ᵢ - x̄)²  (Suma de cuadrados entre grupos)\n",
    "- SSW = Σ(xᵢⱼ - x̄ᵢ)²   (Suma de cuadrados dentro de grupos)\n",
    "- k = número de grupos\n",
    "- N = número total de observaciones\n",
    "- nᵢ = número de observaciones en el grupo i\n",
    "- x̄ᵢ = media del grupo i\n",
    "- x̄ = media general\n",
    "\n",
    "\n",
    "#### ¿Por qué es útil?\n",
    "\n",
    "**Identifica patrones importantes**:\n",
    "   - Nos confirma que no todos los días son iguales\n",
    "   - Nos dice que el día de la semana es un factor importante para predecir ventas\n",
    "\n",
    "\n",
    "#### Ejemplo práctico\n",
    "\n",
    "- **Lunes**: Ventas promedio de 100 unidades\n",
    "- **Viernes**: Ventas promedio de 300 unidades\n",
    "- **Sábado**: Ventas promedio de 400 unidades\n",
    "\n",
    "El ANOVA nos dice que estas diferencias son reales y no producto del azar. Esto significa que:\n",
    "- No es coincidencia que los sábados vendas más\n",
    "- Deberías tener más personal los fines de semana\n",
    "- Deberías preparar más inventario para los días de mayor venta\n",
    "\n",
    "#### ¿Por qué el p-valor es importante?\n",
    "\n",
    "- **p-valor = 0.0000** significa que hay menos de 0.01% de probabilidad de que estas diferencias sean producto del azar\n",
    "- Es como decir: \"Estamos 99.99% seguros de que los días de la semana afectan las ventas\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo días de semana (lunes a viernes) usando números\n",
    "df_semana = df[df['Dia_Semana_Num'].between(1, 5)]\n",
    "\n",
    "# Mapear números a nombres para el gráfico\n",
    "dias_nombres = {1: 'lunes', 2: 'martes', 3: 'miércoles', 4: 'jueves', 5: 'viernes'}\n",
    "df_semana['Dia_Nombre'] = df_semana['Dia_Semana_Num'].map(dias_nombres)\n",
    "\n",
    "# Análisis descriptivo\n",
    "print(\"=== ANÁLISIS POR DÍA DE LA SEMANA (LUNES A VIERNES) ===\")\n",
    "ventas_dia_semana = df_semana.groupby('Dia_Nombre')['Cantidad_Vendida'].agg(['median', 'mean', 'std', 'count'])\n",
    "print(ventas_dia_semana)\n",
    "\n",
    "# Gráfico de ventas por día\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Dia_Nombre', y='Cantidad_Vendida', data=df_semana, \n",
    "            order=['lunes', 'martes', 'miércoles', 'jueves', 'viernes'])\n",
    "plt.title('Distribución de Ventas por Día de la Semana (Lunes a Viernes)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ANOVA para días de semana\n",
    "f_stat, p_val = stats.f_oneway(*[df_semana[df_semana['Dia_Semana_Num'] == dia]['Cantidad_Vendida'] \n",
    "                                for dia in range(1, 6)])\n",
    "print(f\"\\nANOVA para días de semana (Lunes a Viernes): F = {f_stat:.2f}, p-valor = {p_val:.4f}\")\n",
    "\n",
    "# Calcular estadísticas adicionales\n",
    "eta_squared = (f_stat * (len(df_semana.groupby('Dia_Semana_Num')) - 1)) / (f_stat * (len(df_semana.groupby('Dia_Semana_Num')) - 1) + (len(df_semana) - len(df_semana.groupby('Dia_Semana_Num'))))\n",
    "\n",
    "print(\"\\nEstadísticas adicionales:\")\n",
    "print(f\"Eta cuadrado: {eta_squared:.4f} (Tamaño del efecto)\")\n",
    "\n",
    "# Realizar prueba de Levene para homocedasticidad\n",
    "levene_stat, levene_p = stats.levene(*[group['Cantidad_Vendida'].values for name, group in df_semana.groupby('Dia_Semana_Num')])\n",
    "print(f\"Test de Levene: estadístico = {levene_stat:.2f}, p-valor = {levene_p:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretación:\")\n",
    "print(\"- El p-valor extremadamente bajo (< 0.05) indica diferencias significativas en las ventas entre días\")\n",
    "print(\"- El valor F alto sugiere que la variación entre días es mucho mayor que dentro de cada día\")\n",
    "print(f\"- El eta cuadrado (η²) de {eta_squared:.4f} indica que el {eta_squared*100:.1f}% de la varianza en ventas\")\n",
    "# Eta cuadrado (η²) es un indicador estadístico que mide el tamaño del efecto o la fuerza de la relación\n",
    "# entre variables en un análisis de varianza (ANOVA). Técnicamente, representa la proporción de la\n",
    "# variabilidad total en la variable dependiente (ventas) que puede ser explicada por la variable\n",
    "# independiente (en este caso, el día de la semana). Los valores van de 0 a 1, donde:\n",
    "# - η² cercano a 0 indica poco efecto\n",
    "# - η² cercano a 1 indica un efecto muy fuerte\n",
    "# Este coeficiente se suele representar con la letra griega η (eta)\n",
    "print(\"  se explica por el día de la semana\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Variables Categóricas - Interpretación de Resultados\n",
    "\n",
    "## Significancia Estadística\n",
    "\n",
    "### Feriados\n",
    "- **Prueba t**: t = 13.40, p-valor = 0.0000\n",
    "- **Interpretación**: Existe una diferencia estadísticamente significativa en las ventas entre días feriados y no feriados. El p-valor extremadamente bajo (< 0.05) indica que esta diferencia no es producto del azar.\n",
    "\n",
    "### Promociones\n",
    "- **Prueba t**: t = 32.25, p-valor = 0.0000\n",
    "- **Interpretación**: Las promociones de croissants tienen un impacto altamente significativo en las ventas. El valor t más alto (32.25) sugiere que el efecto de las promociones es aún más pronunciado que el de los feriados.\n",
    "\n",
    "### Días de la Semana\n",
    "- **ANOVA**: F = 111.56, p-valor = 0.0000\n",
    "- **Interpretación**: Existen diferencias significativas en las ventas entre los diferentes días de la semana. El alto valor F (111.56) indica que la variabilidad entre días es mucho mayor que la variabilidad dentro de cada día.\n",
    "\n",
    "### Meses\n",
    "- **ANOVA**: F = 38.24, p-valor = 0.0000\n",
    "- **Interpretación**: Se confirma la existencia de patrones estacionales en las ventas a lo largo del año. Aunque el efecto es significativo (p-valor < 0.05), el valor F más bajo en comparación con los días de la semana (38.24 vs 111.56) sugiere que la variación entre meses es menos pronunciada que la variación entre días de la semana.\n",
    "\n",
    "## Conclusiones Principales\n",
    "\n",
    "1. **Efecto de Promociones**: Las promociones de croissants tienen el impacto más fuerte en las ventas, con el valor t más alto (32.25).\n",
    "\n",
    "2. **Patrón Semanal**: La variación entre días de la semana es la más pronunciada (F = 111.56), lo que sugiere que el día de la semana es un factor determinante en las ventas.\n",
    "\n",
    "3. **Efecto de Feriados**: Los días feriados tienen un impacto significativo pero menos pronunciado que las promociones (t = 13.40).\n",
    "\n",
    "4. **Estacionalidad**: Existe un patrón estacional claro, pero su impacto es menos pronunciado que los efectos diarios y semanales.\n",
    "\n",
    "## Recomendaciones\n",
    "\n",
    "1. **Optimización de Promociones**: Dado el fuerte impacto de las promociones, se recomienda analizar en detalle qué tipos de promociones son más efectivas y en qué momentos.\n",
    "\n",
    "2. **Planificación por Día**: La fuerte variación semanal sugiere la necesidad de ajustar los niveles de inventario y personal según el día de la semana.\n",
    "\n",
    "3. **Preparación para Feriados**: Aunque el efecto es menor que las promociones, los feriados siguen siendo importantes y requieren planificación especial.\n",
    "\n",
    "4. **Gestión Estacional**: La estacionalidad mensual, aunque menos pronunciada, debe considerarse en la planificación a largo plazo y en la gestión de inventario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_5.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_3a.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado (Regresión Lineal) y Evaluación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tendencias y estructuras antes del modelado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contra elementos temporales intra serie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos lineales\n",
    "\n",
    "Un modelo lineal es una herramienta estadística que busca explicar una variable dependiente (Y) como una combinación lineal de variables independientes (X). Formalmente se representa como:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ + ε\n",
    "\n",
    "Donde:\n",
    "- Y es la variable dependiente o respuesta\n",
    "- X₁, X₂, ..., Xₖ son las variables independientes o predictoras\n",
    "- β₀ es el intercepto o término constante\n",
    "- β₁, β₂, ..., βₖ son los coeficientes que miden el efecto de cada variable X\n",
    "- ε es el término de error aleatorio\n",
    "\n",
    "El caso más simple es el modelo con un solo regresor:\n",
    "\n",
    "Y = β₀ + β₁X + ε\n",
    "\n",
    "Este modelo asume una relación lineal entre X e Y, donde β₀ representa el valor esperado de Y cuando X=0, y β₁ representa el cambio esperado en Y por cada unidad de cambio en X.\n",
    "\n",
    "Los coeficientes β se estiman típicamente por el método de mínimos cuadrados ordinarios (OLS), que minimiza la suma de los errores al cuadrado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Regresión lineal simple para temperatura\n",
    "X_temp = sm.add_constant(df['Temperatura_Max_Prevista'].astype(float))\n",
    "y = df['Cantidad_Vendida'].astype(float)\n",
    "modelo_temp = sm.OLS(y, X_temp).fit()\n",
    "print(\"=== REGRESIÓN SIMPLE TEMPERATURA ===\")\n",
    "print(modelo_temp.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gráfico de regresión temperatura\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x='Temperatura_Max_Prevista', y='Cantidad_Vendida', data=df)\n",
    "plt.title('Regresión: Ventas vs Temperatura')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.DataFrame({\n",
    "    'Temperatura': df['Temperatura_Max_Prevista'].astype(float),\n",
    "    'Precio_Nuestro': df['Precio_Nuestro'].astype(float),\n",
    "    'Precio_Competencia': df['Precio_Competencia'].astype(float),\n",
    "    'Promocion': df['Promocion_Croissant'].astype(int),\n",
    "    'Feriado': df['Es_Feriado'].astype(int),\n",
    "    'Fin_Semana': df['Es_Fin_Semana'].astype(int),\n",
    "    'Mes': df['Mes'].astype(int),\n",
    "    'Dia_Semana': df['Dia_Semana_Num'].astype(int),\n",
    "    'Cantidad_Vendida': df['Cantidad_Vendida'].astype(float)  # En términos absolutos, pero podríamos normalizar, logaritmizar, etc. \n",
    "})\n",
    "\n",
    "\n",
    "# 5. Regresión polinomial para temperatura\n",
    "df_reg['Temp_Cuad'] = df_reg['Temperatura'] ** 2\n",
    "X_poly = df_reg[['Temperatura', 'Temp_Cuad']]\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "modelo_poly = sm.OLS(df_reg['Cantidad_Vendida'], X_poly).fit()\n",
    "print(\"\\n=== REGRESIÓN POLINOMIAL TEMPERATURA ===\")\n",
    "print(modelo_poly.summary())\n",
    "\n",
    "# Gráfico de regresión polinomial\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x='Temperatura', y='Cantidad_Vendida', data=df_reg, \n",
    "            order=2, scatter_kws={'alpha':0.3})\n",
    "plt.title('Regresión Polinomial: Ventas vs Temperatura')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos lineales con múltiples regresores\n",
    "\n",
    "Extendiendo el modelo lineal simple a múltiples variables independientes, cada regresor adicional aporta información para explicar la variabilidad de Y:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ + ε\n",
    "\n",
    "Al incluir múltiples regresores:\n",
    "\n",
    "- Se asume que la variable a explicar depende de múltiples elementos y no sólo de una variable\n",
    "- Cada coeficiente βᵢ captura el efecto marginal de su variable Xᵢ sobre Y, manteniendo las demás constantes\n",
    "- Variables relevantes adicionales ayudan a reducir el error y mejorar el ajuste\n",
    "- Es posible modelar efectos directos e interacciones entre variables\n",
    "- Permite controlar por factores confusores y aislar efectos individuales\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Regresión múltiple con variables temporales y categóricas\n",
    "# Crear DataFrame para regresión con tipos explícitos\n",
    "df_reg = pd.DataFrame({\n",
    "    'Temperatura': df['Temperatura_Max_Prevista'].astype(float),\n",
    "    'Precio_Nuestro': df['Precio_Nuestro'].astype(float),\n",
    "    'Precio_Competencia': df['Precio_Competencia'].astype(float),\n",
    "    'Promocion': df['Promocion_Croissant'].astype(int),\n",
    "    'Feriado': df['Es_Feriado'].astype(int),\n",
    "    'Fin_Semana': df['Es_Fin_Semana'].astype(int),\n",
    "    'Mes': df['Mes'].astype(int),\n",
    "    'Dia_Semana': df['Dia_Semana_Num'].astype(int),\n",
    "    'Cantidad_Vendida': df['Cantidad_Vendida'].astype(float)  # En términos absolutos, pero podríamos normalizar, logaritmizar, etc. \n",
    "})\n",
    "\n",
    "X_multi = df_reg[['Temperatura', 'Precio_Nuestro', 'Precio_Competencia', \n",
    "                'Promocion', 'Feriado', 'Fin_Semana', 'Mes']]\n",
    "X_multi = sm.add_constant(X_multi)\n",
    "modelo_multi = sm.OLS(df_reg['Cantidad_Vendida'], X_multi).fit()\n",
    "print(\"\\n=== REGRESIÓN MÚLTIPLE ===\")\n",
    "print(modelo_multi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Regresión con variables dummy para días de la semana\n",
    "dias_dummy = pd.get_dummies(df_reg['Dia_Semana'], prefix='Dia', drop_first=True).astype(int)\n",
    "X_dias = pd.concat([X_multi, dias_dummy], axis=1)\n",
    "modelo_dias = sm.OLS(df_reg['Cantidad_Vendida'], X_dias).fit()\n",
    "print(\"\\n=== REGRESIÓN CON DÍAS DE LA SEMANA ===\")\n",
    "print(modelo_dias.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Regresión con interacciones\n",
    "df_reg['Temp_Promo'] = df_reg['Temperatura'] * df_reg['Promocion']\n",
    "df_reg['Precio_Diff'] = df_reg['Precio_Nuestro'] - df_reg['Precio_Competencia']\n",
    "X_inter = df_reg[['Temperatura', 'Precio_Nuestro', 'Precio_Competencia',\n",
    "                'Promocion', 'Feriado', 'Fin_Semana', 'Mes',\n",
    "                'Temp_Promo', 'Precio_Diff']]\n",
    "X_inter = sm.add_constant(X_inter)\n",
    "modelo_inter = sm.OLS(df_reg['Cantidad_Vendida'], X_inter).fit()\n",
    "print(\"\\n=== REGRESIÓN CON INTERACCIONES ===\")\n",
    "print(modelo_inter.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones con Series de Tiempo\n",
    "\n",
    "Las regresiones con series de tiempo usando OLS (Ordinary Least Squares) son un buen punto de partida para analizar relaciones temporales, aunque presentan algunas limitaciones:\n",
    "\n",
    "## Aproximación OLS Básica\n",
    "- Asume independencia entre observaciones\n",
    "- No considera autocorrelación ni heterocedasticidad\n",
    "- Útil para primeras intuiciones y relaciones generales: simplemente introducimos como regresores variables de tiempo\n",
    "\n",
    "## Alternativas más Robustas\n",
    "\n",
    "### DOLS (Dynamic OLS)\n",
    "- Incorpora rezagos y adelantos de variables\n",
    "- Corrige problemas de endogeneidad\n",
    "- Más apropiado para series no estacionarias cointegradas\n",
    "\n",
    "### Fixed Effects (FE)\n",
    "- Controla por heterogeneidad no observada constante en el tiempo\n",
    "- Útil cuando hay efectos específicos por unidad/individuo\n",
    "- Elimina sesgos por variables omitidas invariantes en el tiempo\n",
    "\n",
    "### Random Effects (RE) \n",
    "- Asume efectos aleatorios no correlacionados con regresores\n",
    "- Más eficiente que FE si se cumplen supuestos\n",
    "- Permite estimar efectos de variables invariantes en el tiempo\n",
    "\n",
    "Estas alternativas resuelven problemas de:\n",
    "- Sesgo por endogeneidad y variables omitidas\n",
    "- Ineficiencia por autocorrelación\n",
    "- Inferencia incorrecta por heterocedasticidad\n",
    "\n",
    "Para análisis más rigurosos, se recomienda considerar estas metodologías más avanzadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Regresión con variables de tiempo\n",
    "df_reg['Dia_Anio_Sin'] = np.sin(2 * np.pi * df['Dia_Anio'] / 365)\n",
    "df_reg['Dia_Anio_Cos'] = np.cos(2 * np.pi * df['Dia_Anio'] / 365)\n",
    "X_time = df_reg[['Temperatura', 'Precio_Nuestro', 'Precio_Competencia',\n",
    "                'Promocion', 'Feriado', 'Fin_Semana',\n",
    "                'Dia_Anio_Sin', 'Dia_Anio_Cos']]\n",
    "X_time = sm.add_constant(X_time)\n",
    "modelo_time = sm.OLS(df_reg['Cantidad_Vendida'], X_time).fit()\n",
    "print(\"\\n=== REGRESIÓN CON VARIABLES TEMPORALES ===\")\n",
    "print(modelo_time.summary())\n",
    "\n",
    "# 7. Validación cruzada y métricas\n",
    "X = df_reg[['Temperatura', 'Precio_Nuestro', 'Precio_Competencia',\n",
    "            'Promocion', 'Feriado', 'Fin_Semana', 'Mes']]\n",
    "\n",
    "# Estandarizar variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, df_reg['Cantidad_Vendida'], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar modelo\n",
    "modelo_final = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "y_pred = modelo_final.predict(sm.add_constant(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas\n",
    "print(\"\\n=== MÉTRICAS DEL MODELO ===\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "\n",
    "# Gráfico de residuos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=y_test, lowess=True)\n",
    "plt.title('Gráfico de Residuos')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.ylabel('Residuos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_5.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "    Sobre las regresiones... \n",
    "\n",
    "¿A qué fase del KDD pertenecen esos pasos?\n",
    "\n",
    "Esos pasos pertenecen a la fase de Modelado Exploratorio.\n",
    "\n",
    " -\t\tCuando usamos regresión de manera exploratoria (antes de la división Train/Test formal), no estamos modelando para desplegar todavía.\n",
    " - \t\tEstamos estudiando la relación entre variables.\n",
    " - \t\tEstamos formándonos una idea de cómo es el fenómeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## Modelado y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD: Preprocesamiento - Limpieza Básica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discusión Limpieza:**\n",
    "*   Nuestro dataset sintético está limpio. En un caso real, este paso es crucial.\n",
    "*   Estrategias para nulos en series de tiempo: `ffill` (propagación hacia adelante), `bfill` (hacia atrás), interpolación (`.interpolate()`), imputación con media/mediana (simple, pero puede distorsionar patrones temporales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Fase KDD: Transformación - Ingeniería de Features (1/2) (2/2: test-train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_vars(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos el dataframe para no modificar el original durante el preproceso\n",
    "df_processed = df.copy()\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# --- Crear Dummies para variables categóricas ---\n",
    "\n",
    "\n",
    "### otros ejemplos de variables categóricas:\n",
    "## Categoría de producto (1:CROISSANT, 2:PAN, 3:TARTA, 4:PASTEL)\n",
    "## Evaluación del servicio (Lickert \"Calidad de Servicio\" : muy mala, mala, buena, muy buena)\n",
    "## Evaluación del servicio (Lickert \"Calidad de Servicio\" : 1, 2, 3, 4)\n",
    "############################   no es irrelevante el orden de las categorías, ni el valor base que descartemos!!!! \n",
    "############################   es el caso de UNA VARIABLE CATEGÓRICA ORDINAL O ORDENADA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Seleccionar columnas categóricas a codificar\n",
    "categorical_features = ['Dia_Semana', 'Mes'] \n",
    "\n",
    "print(f\"\\nCodificando variables categóricas: {categorical_features}\")\n",
    "\n",
    "# One hot encoding - OHE\n",
    "# Usamos pd.get_dummies para simplicidad. drop_first=True para evitar multicolinealidad perfecta. <<< \n",
    "df_processed = pd.get_dummies(df_processed, columns=categorical_features, drop_first=True, dtype=int)\n",
    "\n",
    "print(\"\\nNuevas columnas generadas:\")\n",
    "print(df_processed.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_vars(df_processed)\n",
    "# Chequeo si las binarias originales son numéricas (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de protección de flujo:\n",
    "# sirve para que evitar que si en otra situación esas variables vinieran mal definidas, por ejemplo:\n",
    "\t# como object (strings tipo “0”/“1”),\n",
    "\t# como boolean (True/False),\n",
    "\t\n",
    "\n",
    "#vars target:\n",
    "binary_cols = ['Es_Feriado', 'Promocion_Croissant']\n",
    "\n",
    "#transformo en batch:\n",
    "for col in binary_cols:\n",
    "    if col in df_processed.columns and df_processed[col].dtype not in ['int64', 'float64', 'int32', 'float32']:\n",
    "        print(f\"Convirtiendo columna binaria '{col}' a tipo numérico (int)...\")\n",
    "        df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "print(\"\\nTipos de datos finales después de transformaciones:\")\n",
    "df_processed.info()\n",
    "describe_vars(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nota: one-hot encoding sobre una variable categórica\n",
    "\n",
    "\n",
    "\n",
    "En los casos \"Dia_Semana\", valores como \"lunes\", \"martes\", \"miércoles\", etc. se crean tantas columnas como categorías.\n",
    "\n",
    "\n",
    "lunes\tmartes\tmiércoles\n",
    "1\t0\t0\n",
    "0\t1\t0\n",
    "0\t0\t1\n",
    "1\t0\t0\n",
    "\n",
    "la suma de las tres columnas siempre da 1 → colinealidad perfecta.\n",
    "\n",
    "Solución:\n",
    "\t•\tdrop_first=True en pd.get_dummies() elimina una columna (por ejemplo \"lunes\").\n",
    "\t•\tAhora si \"martes\" = 0 y \"miércoles\" = 0, ya sabés que era \"lunes\".\n",
    "\t•\tAsí se evita la colinealidad perfecta: no hay redundancia exacta.\n",
    "\n",
    "Cuando se hace drop_first=True, se toma una categoría como referencia implícita (la primera, 0=lunes). Sin embargo ese drop no la hace desaparecer conceptualmente, sino que los efectos de las demás categorías se interpretan en relación a esa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_vars(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discusión Ingeniería de Features:**\n",
    "*   **One-Hot Encoding:** Convierte categorías (Lunes, Martes...) en columnas binarias (0/1) que los modelos lineales pueden usar.\n",
    "*   **`drop_first=True`:** Elimina una categoría de cada grupo (ej. `Dia_Semana_Monday`) para evitar redundancia. El efecto de la categoría eliminada queda capturado en el intercepto del modelo.\n",
    "*   **Otras Features (Para Futuro):**\n",
    "    *   *Lags:* `df['Ventas_Ayer'] = df['Cantidad_Vendida'].shift(1)` - ¡Muy importante para series de tiempo!\n",
    "    *   *Medias Móviles como Features:* `df['Ventas_MM_7dias'] = df['Cantidad_Vendida'].shift(1).rolling(7).mean()`\n",
    "    *   *Features Cíclicas:* Usar `sin` y `cos` para `Mes` o `Dia_Anio` para capturar la ciclicidad de forma continua.\n",
    "    *   *Interacciones:* `df['Precio_x_Promo'] = df['Precio_Nuestro'] * df['Promocion_Croissant']`\n",
    "    *   *Tendencia Temporal:* Podríamos añadir una columna con un contador simple (`range(len(df))`) o usar `Anio`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Feature Design - Feature Engineering\n",
    "\n",
    "El objetivo es esclarecer o brindar perspectivas alternativas sobre un fenómeno que se expresa en los datos.\n",
    "\n",
    "- Todo dato puede revelar más información que la que su variable central inicialmente sugiere.\n",
    "- Podemos segmentar las transformaciones en:\n",
    "    - Transformaciones necesarias para interpretar el fenómeno.\n",
    "    - Enfoques alternativos que ofrecen lecturas más ricas del fenómeno.\n",
    "    - Transformaciones orientadas a fortalecer el modelo y su interpretación.\n",
    "\n",
    "## Técnicas básicas:\n",
    "\n",
    "- **Categorización**: convertir variables continuas en discretas para captar cambios clave (por ejemplo: muy alto, alto, medio, bajo) o crear agrupaciones específicas (grupo 1, grupo 2, etc.).\n",
    "- **Normalización y Rescalamiento**: ajustar escalas para facilitar el procesamiento y la interpretación.\n",
    "- **Deltas diferenciales y medias móviles**: capturar dinámicas y tendencias temporales o de variación entre observaciones.\n",
    "- **Dummies (One-Hot Encoding)**: representación binaria (0-1, No-Si) de variables (categóricas o continuas) para una mejor integración en modelos estadísticos o de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Tabla de Transformaciones](./aux/tabla_transformaciones.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones Básicas\n",
    "\n",
    "### 1. One-Hot Encoding (OHE) - Dummy Variables\n",
    "**Qué hace**: convierte variables categóricas en variables binarias (0/1).  \n",
    "**Cuándo usarlo**: siempre que quieras incorporar variables categóricas en modelos que requieren entradas numéricas (regresiones, árboles, redes neuronales).\n",
    "\n",
    "### 2. Normalización [0, 1]\n",
    "**Qué hace**: escala los valores de una variable para que estén entre 0 y 1.  \n",
    "**Cuándo usarlo**: útil cuando las variables tienen escalas diferentes y usás modelos sensibles a la magnitud (ej: redes neuronales, KNN).\n",
    "\n",
    "### 3. Estandarización (Media 0, Varianza 1)\n",
    "**Qué hace**: centra la variable en media 0 y la escala para tener desviación estándar 1.  \n",
    "**Cuándo usarlo**: fundamental cuando el modelo asume distribución normal o cuando se usan penalizaciones tipo Lasso/Ridge.\n",
    "\n",
    "### 4. Categorización de Variables Continuas\n",
    "**Qué hace**: convierte variables numéricas en grupos discretos o rangos (bins).  \n",
    "**Cuándo usarlo**: para analizar no linealidades, facilitar interpretaciones o preparar variables para visualización.\n",
    "\n",
    "### 5. Transformación Logarítmica\n",
    "**Qué hace**: aplica logaritmo a los valores, reduciendo el efecto de valores extremos.  \n",
    "**Cuándo usarlo**: útil cuando hay alta asimetría positiva o presencia de outliers grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones Básicas usando Categoria\n",
    "\n",
    "# Copiar el dataframe de trabajo antes de las transformaciones para poder revertir y comparar\n",
    "df_transf = df_processed.copy()\n",
    "\n",
    "\n",
    "# --- 1. One-Hot Encoding sobre Categoria ---\n",
    "print(\"\\nCategorías únicas antes del One-Hot Encoding:\")\n",
    "print(df_transf['Categoria'].unique())\n",
    "\n",
    "df_transf = pd.get_dummies(df_transf, columns=['Categoria'], drop_first=True, dtype=int)\n",
    "\n",
    "print(\"\\nColumnas nuevas después del One-Hot Encoding sobre Categoria:\")\n",
    "print([col for col in df_transf.columns if col.startswith('Categoria_')])\n",
    "\n",
    "# --- 2. Normalización de Cantidad_Vendida ---\n",
    "print(\"\\nCantidad_Vendida - Antes de Normalizar:\")\n",
    "print(df_transf['Cantidad_Vendida'].describe())\n",
    "\n",
    "df_transf['Cantidad_Vendida_normalizada'] = (df_transf['Cantidad_Vendida'] - df_transf['Cantidad_Vendida'].min()) / (df_transf['Cantidad_Vendida'].max() - df_transf['Cantidad_Vendida'].min())\n",
    "\n",
    "print(\"\\nCantidad_Vendida - Después de Normalizar:\")\n",
    "print(df_transf['Cantidad_Vendida_normalizada'].describe())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "df_transf['Cantidad_Vendida'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Original')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "df_transf['Cantidad_Vendida_normalizada'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Normalizada')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Estandarización de Cantidad_Vendida ---\n",
    "df_transf['Cantidad_Vendida_estandarizada'] = (df_transf['Cantidad_Vendida'] - df_transf['Cantidad_Vendida'].mean()) / df_transf['Cantidad_Vendida'].std()\n",
    "\n",
    "print(\"\\nCantidad_Vendida - Después de Estandarizar:\")\n",
    "print(df_transf['Cantidad_Vendida_estandarizada'].describe())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "df_transf['Cantidad_Vendida'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Original')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "df_transf['Cantidad_Vendida_estandarizada'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Estandarizada')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Categorización de Cantidad_Vendida ---\n",
    "df_transf['Cantidad_Vendida_categoria'] = pd.cut(df_transf['Cantidad_Vendida'],\n",
    "                                                  bins=[-np.inf, 100, 500, 1000, np.inf],\n",
    "                                                  labels=['Muy Baja', 'Baja', 'Media', 'Alta'])\n",
    "\n",
    "print(\"\\nDistribución de Categorías en Cantidad_Vendida:\")\n",
    "print(df_transf['Cantidad_Vendida_categoria'].value_counts())\n",
    "\n",
    "# --- 5. Logaritmo de Cantidad_Vendida ---\n",
    "df_transf['Cantidad_Vendida_log'] = np.log1p(df_transf['Cantidad_Vendida'])\n",
    "\n",
    "print(\"\\nCantidad_Vendida - Después de Log-Transformar:\")\n",
    "print(df_transf['Cantidad_Vendida_log'].describe())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "df_transf['Cantidad_Vendida'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Original')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "df_transf['Cantidad_Vendida_log'].hist(bins=30)\n",
    "plt.title('Cantidad_Vendida Log-Transformada')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_5.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n",
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_4.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASE TRANSFORMACIÓN (2/2)\n",
    "\n",
    "## Preparación e implementación de un Modelo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Fase KDD: Modelado y Evaluación (Inicio)\n",
    "\n",
    "\n",
    "#### Fase KDD: Preparación para el Modelado\n",
    "\n",
    "- División temporal Train/Test\n",
    "- Entrenamiento del modelo\n",
    "- Predicción\n",
    "- Evaluación de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. División Train/Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División Temporal del Dataset: ¿Qué hacemos, por qué y cómo?\n",
    "\n",
    "#### ¿Qué hacemos?\n",
    "\n",
    "Dividimos el dataset en dos subconjuntos: \n",
    "- Un **conjunto de entrenamiento (Train)**: contiene los datos más antiguos.\n",
    "- Un **conjunto de prueba (Test)**: contiene los datos más recientes.\n",
    "\n",
    "Utilizamos la **última porción de tiempo (últimos 365 días)** como test, mientras que el resto queda como entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "#### ¿Por qué lo hacemos?\n",
    "\n",
    "La razón principal es simular un escenario realista de **predicción futura**:\n",
    "- En la práctica, cuando hacemos pronósticos o predicciones, siempre intentamos predecir **hacia adelante en el tiempo** usando sólo información **del pasado**.\n",
    "- Si mezcláramos aleatoriamente los datos, estaríamos \"viendo\" el futuro antes de tiempo, y eso generaría **falsas buenas predicciones**.\n",
    "- Dividir temporalmente respeta la **secuencia cronológica** y evita **fugas de información** (data leakage).\n",
    "\n",
    "---\n",
    "\n",
    "#### ¿Cómo lo hacemos?\n",
    "\n",
    "1. **Seleccionamos la columna objetivo** que queremos predecir, en este caso `'Cantidad_Vendida'`.\n",
    "\n",
    "2. **Definimos la fecha de corte**:\n",
    "   - Tomamos el último registro de fecha del dataset.\n",
    "   - Retrocedemos 365 días para establecer el punto donde se separan los datos de entrenamiento y prueba.\n",
    "\n",
    "3. **Creamos dos conjuntos**:\n",
    "   - `X_train`, `y_train`: datos anteriores al corte → sirven para entrenar los modelos.\n",
    "   - `X_test`, `y_test`: datos posteriores o iguales al corte → sirven para evaluar cómo se desempeña el modelo con datos que \"nunca vio\".\n",
    "\n",
    "4. **Visualizamos** la división temporal:\n",
    "   - Mostramos las series de entrenamiento y prueba en un mismo gráfico.\n",
    "   - Marcamos claramente la línea de división para entender visualmente qué parte del tiempo corresponde a cada conjunto.\n",
    "\n",
    "---\n",
    "\n",
    "#### Resumen\n",
    "\n",
    "Esta división **simula condiciones reales de predicción**:\n",
    "- Entrenamos el modelo **sólo con el pasado**.\n",
    "- Evaluamos el modelo **en el futuro**, como ocurre en aplicaciones reales.\n",
    "- Respetamos la lógica temporal de los datos para obtener resultados más **realistas**, **fiables** y **útiles**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables de Predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definir Predictores (X) y Objetivo (y) ---\n",
    "    # como hicimos en el EDA para los modelos de regresión \n",
    "\n",
    "# Excluimos la columna objetivo\n",
    "target_column = 'Cantidad_Vendida'\n",
    "\n",
    "try:\n",
    "    X = df_processed.drop(columns=[target_column])\n",
    "    y = df_processed[target_column]\n",
    "except KeyError:\n",
    "    print(f\"Error: La columna objetivo '{target_column}' no se encuentra en df_processed.\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t•\tCrear una nueva variable llamada X. (X=...) --> X es una copia del df_processed pero sin la columna 'Cantidad_Vendida'.\n",
    "    - \tY = ... --> es sólo la columna 'Cantidad_Vendida'.\n",
    "\n",
    "    Por qué?\n",
    "\n",
    "    Los modelos necesitan separar claramente input (X) de output (y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split del dataset para training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- División Temporal --- \n",
    "# Usaremos el último año (365 días) como conjunto de test\n",
    "test_size_days = 365 \n",
    "split_date = X.index.max() - pd.DateOffset(days=test_size_days - 1)\n",
    "\n",
    "X_train = X[X.index < split_date]\n",
    "X_test = X[X.index >= split_date]\n",
    "y_train = y[y.index < split_date]\n",
    "y_test = y[y.index >= split_date]\n",
    "\n",
    "print(f\"--- División Temporal Realizada ---\")\n",
    "print(f\"Tamaño del set de entrenamiento (X_train): {X_train.shape}\")\n",
    "print(f\"Tamaño del set de entrenamiento (y_train): {y_train.shape}\")\n",
    "print(f\"Tamaño del set de test (X_test):        {X_test.shape}\")\n",
    "print(f\"Tamaño del set de test (y_test):        {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nFecha de inicio de Train: {X_train.index.min().strftime('%Y-%m-%d')}, Fecha de fin de Train: {X_train.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Fecha de inicio de Test:  {X_test.index.min().strftime('%Y-%m-%d')}, Fecha de fin de Test:  {X_test.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Visualizar la división\n",
    "plt.figure(figsize=(16, 4))\n",
    "y_train.plot(label='Train Set')\n",
    "y_test.plot(label='Test Set')\n",
    "plt.axvline(split_date, color='black', linestyle='--', label=f'División ({split_date.strftime(\"%Y-%m-%d\")})')\n",
    "plt.title('División Temporal Train/Test de la Variable Objetivo')\n",
    "plt.ylabel(target_column)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para este ejemplo, tiene sentido: \n",
    "\n",
    "- Último año implica que tengo un corte claro para predecir la tendencia \n",
    "- Implica que mi predicción se va a basar y a entrenar sobre una serie concisa y que \"estira\" (estima) el período siguiente\n",
    "- Los datos sobre la estimación tienen una validación explícita. \n",
    "- ...cuidado, esto no siempre es así. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Cuál es el problema de usar siempre el “último año”?\n",
    "\t•\tPuede estar afectado por eventos atípicos: pandemia, inflación alta, cambios normativos, catástrofes, etc.\n",
    "\t•\tPuede no ser representativo de la dinámica histórica general.\n",
    "\t•\tPuede dificultar la generalización si queremos construir un modelo robusto para distintos momentos históricos.\n",
    "\n",
    "\n",
    "# ![Tabla Training](./aux/tabla_training.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la versión final del dataframe en parquet para usarlo en el despliegue\n",
    "\n",
    "# !pip install pyarrow\n",
    "\n",
    "# Then save and load parquet\n",
    "\n",
    "# Save df_processed as parquet\n",
    "df_processed.to_parquet('df_processed.parquet')\n",
    "\n",
    "# Load df_processed from parquet\n",
    "df_processed = pd.read_parquet('df_processed.parquet')\n",
    "\n",
    "# Describe variables\n",
    "describe_vars(df_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_5.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 4: Modelización y Predicción - Minería de datos\n",
    "\n",
    "\n",
    "## Despliegue predictivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874cc3d",
   "metadata": {},
   "source": [
    "En esta etapa central del proceso KDD, nos centraremos en el despliegue del modelo predictivo para la demanda de croissants. El despliegue implica la implementación del modelo en un entorno de producción donde pueda ser utilizado para realizar predicciones en tiempo real y ayudar en la toma de decisiones del negocio.\n",
    "\n",
    "### Objetivos del Despliegue\n",
    "\n",
    "1. **Preparación del Modelo**:\n",
    "   - Serializar el modelo entrenado para su uso posterior\n",
    "   - Establecer un pipeline de predicción claro y reproducible\n",
    "   - Documentar los requisitos y dependencias\n",
    "\n",
    "2. **Implementación**:\n",
    "   - Crear funciones de predicción fáciles de usar\n",
    "   - Establecer un flujo de trabajo para nuevas predicciones\n",
    "   - Asegurar la consistencia en el procesamiento de datos\n",
    "\n",
    "3. **Monitoreo y Mantenimiento**:\n",
    "   - Definir métricas de monitoreo del rendimiento\n",
    "   - Establecer procedimientos de actualización del modelo\n",
    "   - Implementar validaciones de calidad de datos\n",
    "\n",
    "### Consideraciones Importantes\n",
    "\n",
    "- **Reproducibilidad**: Asegurar que el proceso sea reproducible en diferentes entornos\n",
    "- **Escalabilidad**: Preparar el sistema para manejar diferentes volúmenes de predicciones\n",
    "- **Mantenibilidad**: Facilitar la actualización y mejora continua del modelo\n",
    "- **Documentación**: Proporcionar documentación clara para usuarios y desarrolladores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: Decision Trees\n",
    "\n",
    "# ![Tabla de modelos de decision trees](./aux/tabla_modelos_dt1.png)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Variables predictoras y target\n",
    "features = [\n",
    "    'Temperatura_Max_Prevista',\n",
    "    'Promocion_Croissant',\n",
    "    'Precio_Nuestro',\n",
    "    'Precio_Competencia',\n",
    "    'Es_Feriado',\n",
    "    'Es_Fin_Semana',\n",
    "    'Dia_Semana_jueves',\n",
    "    'Dia_Semana_lunes',\n",
    "    'Dia_Semana_martes',\n",
    "    'Dia_Semana_miércoles',\n",
    "    'Dia_Semana_sábado',\n",
    "    'Dia_Semana_viernes',\n",
    "    'Mes_2', 'Mes_3', 'Mes_4', 'Mes_5', 'Mes_6', 'Mes_7',\n",
    "    'Mes_8', 'Mes_9', 'Mes_10', 'Mes_11', 'Mes_12'\n",
    "]\n",
    "target = 'Cantidad_Vendida'\n",
    "\n",
    "X = df_processed[features]\n",
    "y = df_processed[target]\n",
    "\n",
    "# División en entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenamiento del árbol de decisión\n",
    "tree_model = DecisionTreeRegressor(max_depth=4, random_state=42,  max_leaf_nodes=12)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué hace el modelo DecisionTreeRegressor?\n",
    "\n",
    "Un DecisionTreeRegressor es un modelo de aprendizaje supervisado (Supervised Machine Learning) que predice valores numéricos continuos. En este caso, el valor a predecir es Cantidad_Vendida de croissants. Funciona dividiendo el espacio de datos en particiones jerárquicas basadas en reglas del tipo:\n",
    "\t•\t¿Precio_Nuestro < 80?\n",
    "\t•\t¿Es_Fin_Semana == 1?\n",
    "\t•\t¿Temperatura_Max_Prevista > 27?\n",
    "\n",
    "Cada nodo del árbol representa una condición que separa el dataset según un umbral de una variable. Los nodos terminales (hojas) contienen el valor promedio de la variable objetivo (Cantidad_Vendida) en esa partición.\n",
    "\n",
    "⸻\n",
    "\n",
    "### ¿Para qué sirve?\n",
    "\tExplicabilidad: al ser gráfico e interpretable, es ideal para entender la lógica detrás de las predicciones.\n",
    "\tEntrenamiento base: sirve como modelo simple antes de usar RandomForest, que es una técnica apoyada sobre un conjunto de árboles como este.\n",
    "\n",
    "⸻\n",
    "\n",
    "### ¿Qué muestra?\n",
    "\n",
    "Al visualizar el árbol:\n",
    "\t•\tCada nodo muestra una regla: p. ej. Promoción <= 0.5\n",
    "\t•\tLos nodos hijos indican la partición resultante si la condición se cumple o no.\n",
    "\t•\tLos valores en las hojas indican la predicción promedio de Cantidad_Vendida para ese subconjunto.\n",
    "\n",
    "También se incluye el número de observaciones en cada nodo y la desviación estándar.\n",
    "\n",
    "⸻\n",
    "\n",
    "### ¿Cómo se interpreta?\n",
    "\t•\tUn árbol permite ver las reglas más importantes que dividen los datos.\n",
    "\t•\tEl orden de las condiciones da una idea de qué variables son más relevantes en las decisiones del modelo.\n",
    "\t•\tSi una hoja tiene muchas observaciones y una predicción alejada del promedio global, implica señales de overfitting (relacionado usualmente con la profundidad (parámetro max_depth=4, o leafs mínimas, o relación sample-vars-tree).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# indicadores básicos del modelo\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(\"\\n========\\n\",\"MSE del modelo (dv std predicto versus validation):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred), \"\\n========\\n\")\n",
    "\n",
    "# Visualización en texto del arbol \n",
    "tree_text = export_text(tree_model, feature_names=features, decimals=1)\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ATENCIÓN:\n",
    "\n",
    "Los árboles de decisión, como `DecisionTreeRegressor` de scikit-learn, siempre dividen el conjunto de datos mediante **reglas binarias**, es decir, cada nodo se divide en dos ramas según una condición.\n",
    "\n",
    "Veamos los distintos tipos de variables al construir los splits:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Variables binarias (0 y 1):**\n",
    "\n",
    "El árbol no reconoce que la variable es binaria. La trata como continua y evalúa posibles umbrales. Naturalmente, el único corte útil es:\n",
    "\n",
    "$Variable \\leq 0.5$ (equivale a: Variable == 0)\n",
    "\n",
    "Aunque el resultado es correcto, la interpretación humana es más clara si se expresa como igualdad.\n",
    "\n",
    "*- Interpretar las condiciones sobre variables binarias (como `<= 0.5`) como igualdades lógicas*\n",
    "\n",
    "---\n",
    "\n",
    "**2. Variables continuas (por ejemplo, temperatura entre 3 y 35):**\n",
    "\n",
    "El árbol evalúa todos los puntos posibles entre valores consecutivos. Por cada posible umbral $t$, calcula:\n",
    "\n",
    "$MSE_{split}(t) = \\frac{n_L}{n} \\cdot MSE_L + \\frac{n_R}{n} \\cdot MSE_R$\n",
    "\n",
    "y selecciona el corte que minimiza ese valor. El resultado es una condición binaria del tipo:\n",
    "\n",
    "$Temperatura \\leq 18.5$\n",
    "\n",
    "\n",
    "Algo un poco más detallado:\n",
    "\n",
    "### Cómo se determinan los umbrales de corte para variables continuas en árboles de decisión\n",
    "\n",
    "Cuando una variable es continua, el árbol **no evalúa todos los valores reales posibles** como umbrales (eso sería infinito). En cambio, sigue una estrategia concreta:\n",
    "\n",
    "1. Ordena las observaciones según los valores de la variable continua $x_j$.\n",
    "2. Considera como umbrales candidatos todos los **puntos medios entre pares de valores consecutivos distintos**.\n",
    "\n",
    "Si los valores únicos ordenados de $x_j$ son $x_1, x_2, \\dots, x_n$, los umbrales que el árbol evalúa son:\n",
    "\n",
    "$t_k = \\frac{x_k + x_{k+1}}{2}, \\quad \\text{para } k = 1, 2, \\dots, n - 1$\n",
    "\n",
    "\n",
    "#### Ejemplo:\n",
    "\n",
    "Si $x = [3, 7, 12, 20]$, entonces los cortes candidatos son:\n",
    "\n",
    "- $t_1 = \\frac{3 + 7}{2} = 5$\n",
    "- $t_2 = \\frac{7 + 12}{2} = 9.5$\n",
    "- $t_3 = \\frac{12 + 20}{2} = 16$\n",
    "\n",
    "##### Evaluación del mejor corte\n",
    "\n",
    "Para cada candidato $t$, el árbol divide el conjunto de datos en dos subconjuntos:\n",
    "\n",
    "- $S_L = \\{ x_i \\leq t \\}$\n",
    "- $S_R = \\{ x_i > t \\}$\n",
    "\n",
    "y calcula el siguiente valor de error:\n",
    "\n",
    "$MSE_{split}(t) = \\frac{n_L}{n} \\cdot MSE_L + \\frac{n_R}{n} \\cdot MSE_R$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $n$ es el número total de observaciones\n",
    "- $n_L$ y $n_R$ son los tamaños de los subconjuntos izquierdo y derecho\n",
    "- $MSE_L$ y $MSE_R$ son los errores cuadráticos medios dentro de cada subconjunto\n",
    "\n",
    "El árbol elige el umbral $t$ que **minimiza este valor** (por ej: `Temperatura_Max_Prevista <= 18.9`).\n",
    "\n",
    "    Entonces:\n",
    "    - Las divisiones del árbol **siempre son binarias**, incluso en variables continuas.\n",
    "    - Los cortes se prueban **sólo en puntos medios entre valores únicos consecutivos**.\n",
    "    - Esto garantiza que el espacio de búsqueda sea finito, exhaustivo y computacionalmente viable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. Variables categóricas con más de dos valores (por ejemplo: A, B, C, D, E):**\n",
    "\n",
    "Scikit-learn **no admite directamente variables categóricas**. \n",
    "\n",
    "Por tanto, deben transformarse:\n",
    "\n",
    "- **OHE - Codificación one-hot (es decir, dummies, o dummy variables, variables 0,1):**\n",
    "\n",
    "    crea una columna por categoría (`cat_A`, `cat_B`, etc.). El árbol realiza splits binarios sobre cada dummy:\n",
    "\n",
    "    $cat\\_B \\leq 0.5$ (equivale a: categoría distinta de B)\n",
    "\n",
    "- **Codificación ordinal (label encoding):** no se recomienda salvo que el orden tenga significado (poco-mucho, bajo-alto, bueno-malo y ese tipo de gradientes), ya que el árbol evaluará cortes continuos... esto hará que, por ejemplo, encontremos cortes como:\n",
    "\n",
    "$Categoría \\leq 2.5$\n",
    "\n",
    "lo cual puede inducir a errores de interpretación. Lo más recomendable a no ser que haya una hipótesis particular de trabajo, es la transformación OHE. \n",
    "*- No usar variables categóricas sin transformar*\n",
    "*- Aplicar codificación one-hot a todas las variables nominales no ordinales*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicadores básicos del modelo\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(\"\\n========\\n\",\"MSE del modelo (dv std predicto versus validation):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred), \"\\n========\\n\")\n",
    "\n",
    "# Visualización en texto del arbol \n",
    "tree_text = export_text(tree_model, feature_names=features, decimals=2)\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del árbol (versión rústica, puede haber problemas de superposición)\n",
    "plt.figure(figsize=(24, 14))\n",
    "plot_tree(tree_model, feature_names=features, filled=True, rounded=True, fontsize=13)\n",
    "plt.title(\"Árbol de decisión para predicción de cantidad vendida de croissants\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del árbol (versión elaborada, elimina superposiciones y genera más control)\n",
    "\n",
    "output_dir = \"model_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Rutas de salida\n",
    "dot_path = os.path.join(output_dir, \"arbol_croissants.dot\")\n",
    "png_path = os.path.join(output_dir, \"arbol_croissants.png\")\n",
    "\n",
    "# Exportar el árbol a .dot\n",
    "export_graphviz(\n",
    "    tree_model,\n",
    "    out_file=dot_path,\n",
    "    feature_names=features,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# Convertir .dot a .png\n",
    "subprocess.run([\"dot\", \"-Tpng\", dot_path, \"-o\", png_path], check=True)\n",
    "\n",
    "print(f\"Árbol exportado en:\\n- DOT: {dot_path}\\n- PNG: {png_path}\")\n",
    "# Mostrar imagen generada\n",
    "display(Image(filename=\"model_outputs/arbol_croissants.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicadores básicos del modelo\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(\"\\n========\\n\",\"MSE del modelo (dv std predicto versus validation):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred), \"\\n========\\n\")\n",
    "\n",
    "# Visualización en texto del arbol \n",
    "tree_text = export_text(tree_model, feature_names=features, decimals=2)\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"><img src=\"./aux/kdd_6.png\" width=\"400\" height=\"300\" style=\"filter: drop-shadow(5px 5px 5px rgba(0,0,0,0.3));\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 5: Evaluación e interpretación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del MSE (primera aproximación, en el próximo encuentro veremos estrategias complementarias)\n",
    "\n",
    "El Error Cuadrático Medio (MSE) nos devuelve un valor absoluto (ej. 234,9), pero ¿qué quiere decir? ¿Es bueno? ¿Es malo?\n",
    "\n",
    "Por sí solo, el MSE es difícil de interpretar porque depende de la escala de la variable objetivo. Por eso, se utilizan métricas complementarias que permiten contextualizar ese error en términos relativos o porcentuales.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **nRMSE** (Normalized Root Mean Squared Error)\n",
    "\n",
    "$nRMSE = \\frac{\\sqrt{MSE}}{\\sigma_y}$\n",
    "\n",
    "- **Qué mide:** el RMSE mide cuánto se desvía (el error de) la predicción en relación al desvío estándar de la variable real.\n",
    "- **Interpretación:** expresa el error como fracción de la variabilidad natural de los datos.  \n",
    "  Por ejemplo, si $nRMSE = 0.2$, el error es el 20% de la dispersión típica de los valores reales.\n",
    "\n",
    "¿Cuánto del total de la variación en y_test (dato real) no estoy pudiendo predecir correctamente?\n",
    "\n",
    "Un $nRMSE < 1$ quiere decir que el error promedio de mis predicciones es menor que la variación total del fenómeno. ¿Por qué es esto posible? **Porque el modelo logra explicar parte de esa variación sistemáticamente usando las variables X.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **MASE** (Mean Absolute Scaled Error)\n",
    "\n",
    "$MASE = \\frac{MAE_{modelo}}{MAE_{naïve}}$\n",
    "\n",
    "- **Qué mide:** compara el error absoluto medio del modelo con el que se obtendría si se predijera siempre la media de los valores reales.\n",
    "- **Interpretación:** evalúa si el modelo, usando variables explicativas, logra reducir el error frente a una estrategia que no usa ninguna información.\n",
    "  - $MASE < 1$: el modelo mejora respecto a predecir siempre la media.\n",
    "  - $MASE = 1$: el modelo tiene el mismo error que predecir la media.\n",
    "  - $MASE > 1$: el modelo es peor que simplemente predecir el promedio.\n",
    "- **Importante:** no se compara la media de las predicciones, sino el error promedio respecto de los valores reales. No se espera que el modelo \"reproduzca\" el fenómeno, sino que lo prediga mejor que una estrategia vacía. Esa predicción implica las mejoras usadas por las features (variables X, o regresores)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **SMAPE** (Symmetric Mean Absolute Percentage Error)\n",
    "\n",
    "$SMAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}$\n",
    "\n",
    "- **Qué mide:** el error porcentual promedio entre la predicción y el valor real.\n",
    "- **Ventajas:** está acotado entre 0 y 100, y es más robusto cuando $y \\to 0$ que el MAPE.\n",
    "- **Interpretación:** un $SMAPE = 15\\%$ indica que, en promedio, la diferencia entre predicción y valor real es del 15% del valor medio de ambos.\n",
    "- \tSi el modelo captura toda la varianza de y (o sea, R² = 1), entonces el error es cero. El error sólo puede ser == 0 si la predicción es perfecta, es decir, consideramos todos los elementos (visibles, invisibles, directos e indirectos) que afectan cómo se mueve \"y_test\".\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicadores adicionales de calidad del modelo\n",
    "\n",
    "# nRMSE: RMSE normalizado por el desvío estándar de y_test\n",
    "# Normalized Root Mean Squared Error - nRMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_y = np.std(y_test)\n",
    "nrmse = rmse / std_y\n",
    "print(\"\\nIndicadores de calidad del modelo:\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"nRMSE (RMSE / STD):\", round(nrmse, 4))\n",
    "print(\"Interpretación: El error promedio representa un {:.1f}% de la variabilidad natural de las ventas\".format(nrmse*100))\n",
    "print(\"Un nRMSE < 1 indica que el modelo logra explicar parte de la variación usando las variables predictoras\")\n",
    "\n",
    "\n",
    "# MASE: compara el MAE del modelo contra un pronóstico ingenuo (media)\n",
    "# Mean Absolute Scaled Error - MASE\n",
    "mae_modelo = mean_absolute_error(y_test, y_pred)\n",
    "mae_naive = mean_absolute_error(y_test, np.full_like(y_test, np.mean(y_test)))\n",
    "mase = mae_modelo / mae_naive\n",
    "print(\"\\nMASE (MAE modelo / MAE naive):\", round(mase, 4))\n",
    "if mase < 1:\n",
    "    print(\"Interpretación: El modelo mejora en un {:.1f}% respecto a predecir la media de y_test\".format((1-mase)*100))\n",
    "    print(\"o: El uso de mis features (variables X) mejoran en un {:.1f}% la predicción respecto al error MAE de la media de y_test\".format((1-mase)*100))\n",
    "\n",
    "else:\n",
    "    print(\"Interpretación: El modelo es peor que simplemente predecir la media porque es mayor a 1\")\n",
    "\n",
    "# SMAPE: error porcentual absoluto simétrico\n",
    "# Symmetric Mean Absolute Percentage Error - SMAPE\n",
    "smape = np.mean(\n",
    "    np.abs(y_test - y_pred) / ((np.abs(y_test) + np.abs(y_pred)) / 2)\n",
    ") * 100\n",
    "print(\"\\nSMAPE (%):\", round(smape, 2))\n",
    "print(\"Interpretación: En promedio, la diferencia entre predicción y valor real es del {:.1f}%\".format(smape))\n",
    "print(\"O: Frente a una predicción perfecta (0% de error) el error del modelo es del {:.1f}% (modelo tan bueno como tirar una moneda = error 100%)\".format(smape))\n",
    "print(\"Nota: si el error = 100%, el error promedio igualó al tamaño del fenómeno que se quería predecir. El modelo es inútil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicadores básicos del modelo\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n========\",\"\\nMSE del modelo:\", mean_squared_error(y_test, y_pred))\n",
    "# nRMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "std_y = np.std(y_test)\n",
    "nrmse = rmse / std_y\n",
    "print(\"nRMSE:\", round(nrmse, 4))\n",
    "# MASE\n",
    "mae_modelo = mean_absolute_error(y_test, y_pred)\n",
    "mae_naive = mean_absolute_error(y_test, np.full_like(y_test, np.mean(y_test)))\n",
    "mase = mae_modelo / mae_naive\n",
    "print(\"MASE:\", round(mase, 4))\n",
    "# SMAPE\n",
    "smape = np.mean(\n",
    "    np.abs(y_test - y_pred) / ((np.abs(y_test) + np.abs(y_pred)) / 2)\n",
    ") * 100\n",
    "print(\"SMAPE (%):\", round(smape, 2))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "print(\"========\\n\")\n",
    "\n",
    "\n",
    "# Visualización en texto del arbol \n",
    "tree_text = export_text(tree_model, feature_names=features, decimals=2)\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Qué buscamos en la interpretación\n",
    "\n",
    "### 1. Segmentos con alta demanda esperada  \n",
    "**(Identificables visualmente en las hojas finales o vía SHAP/Path Extraction)**  \n",
    "- Sirven para previsión, planificación de stock y logística  \n",
    "    - `Promoción + Feriado + Temperatura alta → 240.00 unidades`\n",
    "\n",
    "### 2. Segmentos con baja demanda esperada  \n",
    "**(Visualización directa del árbol o filtrado por path predictivo con menor valor)**  \n",
    "- Sirven para evitar sobreproducción o activar acciones comerciales  \n",
    "    - `Sin promoción + No viernes + Temperatura baja → 67.08`\n",
    "\n",
    "### 3. Sensibilidad a ciertas variables  \n",
    "**(Evaluable mediante análisis de Gain o reducción local de MSE por split, o con SHAP local)**  \n",
    "- Casos donde una sola variable cambia fuertemente la predicción  \n",
    "    - `entre Es_Feriado = 0 y Es_Feriado = 1 con todo igual, pasás de 95.43 a 150.31`\n",
    "\n",
    "### 4. Efectos acumulativos  \n",
    "**(Extraíbles vía análisis de paths completos o combinaciones de condiciones en SHAP)**  \n",
    "- ¿Qué pasa cuando se combinan variables favorables?  \n",
    "    - Promoción + Fin de semana + Temperatura > 15.95 → 198.62  \n",
    "    - + Feriado → 240.00\n",
    "\n",
    "### 5. Umbrales de decisión prácticos  \n",
    "**(Derivados directamente de los cortes en cada nodo del árbol: temperatura, precio, etc.)**  \n",
    "- Temperatura ≤ o > 18  \n",
    "- Precio ≤ 1.46  \n",
    "- Son cortes concretos que te permiten anticipar cambios en el nivel de ventas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Métricas  para interpretar la estructura del árbol de decisión\n",
    "# =============================================================================\n",
    "\n",
    "# importancia de variables (gain), \n",
    "# reducción de error (impureza), \n",
    "# caminos de decisión por muestra (paths) y \n",
    "# valores SHAP.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Importancia de las variables (gain acumulado del árbol) >>> Métrica elemental e indispensable\n",
    "# Esta métrica mide cuánto reduce el MSE cada variable a lo largo de todo el árbol.\n",
    "importancia = pd.DataFrame({\n",
    "    'Variable': features,\n",
    "    'Importancia': tree_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nRESULTADOS DEL DECISION TREE\")\n",
    "\n",
    "print(\"\\n--- Importancia de variables (Gain total por split) ---\")\n",
    "print(importancia)\n",
    "# ¿Cuáles son las variables que más impactan en la predicción de ventas a nivel global del modelo?\n",
    "\n",
    "\n",
    "\n",
    "# 2. Importancia por permutación (Permutation Importance)\n",
    "# Evalúa cuánto empeora la performance del modelo si se desordena cada variable.\n",
    "# Captura interacciones y dependencias más allá del árbol construido.\n",
    "perm_importance = permutation_importance(tree_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "importancia_perm = pd.DataFrame({\n",
    "    'Variable': features,\n",
    "    'Importancia': perm_importance.importances_mean\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\n--- Importancia por permutación (Permutation Importance) ---\")\n",
    "print(importancia_perm)\n",
    "# ¿Qué variables son realmente importantes cuando se consideran las interacciones entre ellas?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Caminos de decisión (Decision Paths)\n",
    "# Extrae las condiciones que se cumplen para cada muestra de test y el valor predicho\n",
    "# Útil para identificar qué combinaciones activan ciertas predicciones\n",
    "print(\"\\n--- Caminos de decisión por observación ---\")\n",
    "tree_ = tree_model.tree_\n",
    "feature_names = np.array(features)\n",
    "\n",
    "def extraer_paths(model, X):\n",
    "    paths = []\n",
    "    for i in range(X.shape[0]):\n",
    "        node_index = model.decision_path(X.iloc[[i]]).indices\n",
    "        conds = []\n",
    "        for node_id in node_index:\n",
    "            if node_id == tree_.node_count - 1:\n",
    "                continue\n",
    "            feature = tree_.feature[node_id]\n",
    "            threshold = tree_.threshold[node_id]\n",
    "            if X.iloc[i, feature] <= threshold:\n",
    "                conds.append(f\"{feature_names[feature]} <= {threshold:.2f}\")\n",
    "            else:\n",
    "                conds.append(f\"{feature_names[feature]} > {threshold:.2f}\")\n",
    "        paths.append(\" ∧ \".join(conds))\n",
    "    return paths\n",
    "\n",
    "# Generamos todos los paths\n",
    "decision_paths = extraer_paths(tree_model, X_test)\n",
    "\n",
    "# Muestras individuales (las primeras 5 en orden natural)\n",
    "print(\"\\n--- Caminos de decisión por observación (primeras 15 muestras) ---\")\n",
    "for i, path in enumerate(decision_paths[:15]):\n",
    "    print(f\"Muestra {i+1} (predicción: {round(y_pred[i], 2)}):\")\n",
    "    print(path)\n",
    "\n",
    "print(\"\\n\\nSignifica que, dadas las condiciones observadas en esa muestra (Promoción = 0, Es fin de semana = 1, etc.), el árbol predice que se venderán aproximadamente 107.05 unidades.\")\n",
    "\n",
    "# Muestras ordenadas por valor predicho (descendente)\n",
    "print(\"\\n--- Caminos de decisión por predicción (ordenados) ---\")\n",
    "sorted_indices = np.argsort(-y_pred)  # mayor a menor\n",
    "\n",
    "for rank, i in enumerate(sorted_indices[:5]):\n",
    "    print(f\"Muestra {i} (rank {rank+1}, predicción: {round(y_pred[i], 2)}):\")\n",
    "    print(decision_paths[i])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. SHAP values para árbol\n",
    "# Mide la contribución exacta de cada variable a cada predicción\n",
    "explainer = shap.TreeExplainer(tree_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "## Graph Shap\n",
    "\n",
    "# Filtro: eliminar variables con impacto nulo\n",
    "shap_importancia = np.abs(shap_values).mean(axis=0)\n",
    "variables_con_impacto = shap_importancia > 0\n",
    "X_shap = X_test.loc[:, variables_con_impacto]\n",
    "features_shap = X_shap.columns.tolist()\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"gray_to_red\", [\"#d3d3d3\", \"#ff0000\"])\n",
    "\n",
    "# Visualización SHAP con plot ajustado\n",
    "shap.summary_plot(\n",
    "    shap_values[:, variables_con_impacto],\n",
    "    X_shap,\n",
    "    feature_names=features_shap,\n",
    "    plot_size=(8, 5),\n",
    "    cmap=custom_cmap\n",
    ")\n",
    "# Nota: la visualización SHAP abre una figura interactiva. Si corrés esto en un entorno no interactivo (como consola), podrías no verla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\t•\tLa mayoría de los datos de test no tienen promoción activa (Promocion_Croissant = 0), por eso hay más puntos azules.\n",
    "\t•\tPocas observaciones tienen valor alto (mayor a 1), de ahí los pocos puntos rojos.\n",
    "    \t•\tCuando hay promoción (puntos rojos), el impacto sobre la predicción es muy alto y positivo.\n",
    "\t•\tPero como hay menos casos con promoción, la frecuencia de esos puntos es menor.\n",
    "\t•\tEn cambio, la ausencia de promoción (azul) no baja tanto la predicción, sino que la mantiene neutral o levemente baja.\n",
    "     frecuencia ≠ importancia, y eso se ve en cómo se dispersan los puntos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recordemos: \n",
    "\n",
    "En cada nodo sucede esto:\n",
    "$$\n",
    "\\text{Split óptimo} = \\arg\\min_{\\text{split}} \\left( \\frac{n_{\\text{izq}}}{n} \\cdot \\text{Var}_{\\text{izq}}(y) + \\frac{n_{\\text{der}}}{n} \\cdot \\text{Var}_{\\text{der}}(y) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "### 1. Importancia de variables (Gain total por split)\n",
    "\n",
    "| Variable | Importancia |\n",
    "|----------|------------|\n",
    "| Promocion_Croissant | 0.4228 |\n",
    "| Es_Fin_Semana | 0.2836 |\n",
    "| Temperatura_Max_Prevista | 0.1992 |\n",
    "| Es_Feriado | 0.0700 |\n",
    "| Dia_Semana_viernes | 0.0182 |\n",
    "| Precio_Nuestro | 0.0062 |\n",
    "| Resto de variables | 0.0000 |\n",
    "\n",
    "Este indicador muestra cuánto contribuye cada variable a reducir el error (MSE) en los splits del árbol. Refleja la importancia estructural del modelo entrenado.\n",
    "\n",
    "Interpretación:\n",
    "- Promocion_Croissant es la variable dominante: representa el 42% de la ganancia total del modelo. Es el principal driver.\n",
    "- Es_Fin_Semana y Temperatura_Max_Prevista también tienen peso significativo (28% y 20%).\n",
    "- Es_Feriado tiene impacto, pero menor (7%).\n",
    "- Las variables de día y mes, salvo viernes, no fueron usadas en ningún split, o tienen una contribución nula.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Permutation Importance\n",
    "\n",
    "| Variable | Importancia |\n",
    "|----------|------------|\n",
    "| Promocion_Croissant | 0.9298 |\n",
    "| Es_Fin_Semana | 0.3939 |\n",
    "| Temperatura_Max_Prevista | 0.2790 |\n",
    "| Es_Feriado | 0.1076 |\n",
    "| Dia_Semana_viernes | 0.0179 |\n",
    "| Precio_Nuestro | 0.0073 |\n",
    "| Resto de variables | 0.0000 |\n",
    "\n",
    "#### Qué es:\n",
    "\n",
    "Es un método agnóstico del modelo que evalúa cuán dependiente es el modelo de cada variable para hacer predicciones correctas.\n",
    "\n",
    "Para cada variable, se desordena aleatoriamente su columna en X_test. Esto rompe la relación entre esa variable y y, manteniendo el resto igual.\n",
    "\n",
    "Se vuelve a predecir usando el modelo y se calcula cuánto empeora el error respecto al original. Este procedimiento se repite varias veces (n_repeats=10) para reducir ruido, y se promedia el impacto.\n",
    "\n",
    "\t•\tSi el error aumenta mucho al desordenar una variable → esa variable es importante para el modelo.\n",
    "\t•\tSi el error no cambia → el modelo no depende de esa variable.\n",
    "    \tCaptura el impacto real sobre la predicción, no sobre la estructura.\n",
    "\n",
    " Es un test independiente de la estructura del árbol y más sensible a interacciones o efectos marginales no reflejados en splits explícitos.\n",
    "\n",
    "#### Interpretación:\n",
    "- Confirma el resultado anterior: Promoción es absolutamente determinante, con casi el 93% de la importancia relativa.\n",
    "- Las demás mantienen su peso, aunque la magnitud relativa cambia un poco.\n",
    "- Refuerza la conclusión de que muchas variables disponibles no aportan nada útil al modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Caminos de decisión por observación (Decision Paths)\n",
    "\n",
    "¿Qué conjunción de condiciones es necesaria para llegar a cierto resultado? Este análisis es estructural, no marginal ni de variables aisladas.\n",
    "\n",
    "Muestra 1: Promocion_Croissant <= 0.50 ∧ Es_Fin_Semana > 0.50 ∧ Temperatura_Max_Prevista <= 18.00  \n",
    "Muestra 2: Promocion_Croissant > 0.50 ∧ Temperatura_Max_Prevista > 15.95 ∧ Es_Fin_Semana <= 0.50 ∧ Es_Feriado <= 0.50  \n",
    "\n",
    "|--- Promocion_Croissant >  0.50  \n",
    "|   |--- Temperatura_Max_Prevista >  15.95  \n",
    "|   |   |--- Es_Fin_Semana <= 0.50  \n",
    "|   |   |   |--- Es_Feriado <= 0.50  \n",
    "|   |   |   |   |--- value: [161.56]  \n",
    "\n",
    "Muestra 3: Promocion_Croissant > 0.50 ∧ Temperatura_Max_Prevista > 15.95 ∧ Es_Fin_Semana > 0.50  \n",
    "Muestra 4: Promocion_Croissant > 0.50 ∧ Temperatura_Max_Prevista > 15.95 ∧ Es_Fin_Semana <= 0.50 ∧ Es_Feriado <= 0.50  \n",
    "Muestra 5: Promocion_Croissant <= 0.50 ∧ Es_Fin_Semana <= 0.50 ∧ Temperatura_Max_Prevista > 18.95 ∧ Es_Feriado <= 0.50  \n",
    "\n",
    "… A partir de los splits del árbol, se reconstruyen los caminos secuenciales de decisiones que explican cada predicción. Esto permite identificar qué combinaciones de variables **disparan mayores niveles de ventas estimadas**.\n",
    "\n",
    "#### Qué significa:\n",
    "\n",
    "Este análisis muestra las reglas específicas que activan cada predicción. Cada observación se clasifica siguiendo un camino secuencial de decisiones.  \n",
    "Permite seleccionar y priorizar combinaciones de condiciones con alto impacto esperado en ventas, según la lógica del modelo entrenado.\n",
    "\n",
    "#### Interpretación:\n",
    "- Permite rastrear por qué una predicción dio el resultado que dio.\n",
    "- Identifica combinaciones reales de condiciones, no solo variables individuales.\n",
    "- Identifica combinaciones específicas de alto impacto. \n",
    "---\n",
    "\n",
    "\n",
    "### 4. SHAP values\n",
    "\n",
    "(Salida visual esperada: summary_plot de SHAP)\n",
    "\n",
    "#### Qué significa:\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) cuantifica la contribución de cada variable a cada predicción individual.\n",
    "- En el gráfico summary_plot, el eje X muestra el impacto (positivo o negativo) de cada variable sobre la predicción.\n",
    "- Los colores muestran si el valor de esa variable era alto o bajo.\n",
    "\n",
    "#### Interpretación esperada:\n",
    "- Promoción debería aparecer con la mayor dispersión de impacto en el eje horizontal.\n",
    "- El impacto de Es_Fin_Semana, Temperatura_Max_Prevista, y Es_Feriado debería verse también, con contribuciones claramente diferenciadas.\n",
    "- Las variables que no aportaron nada no deberían figurar con dispersión en el gráfico (concentradas en cero).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos los escenarios de decisión\n",
    "\n",
    "\n",
    "tree_ = tree_model.tree_\n",
    "feature_names_array = np.array(features)\n",
    "\n",
    "nodos_pendientes = [(0, [])]  # nodo raíz, sin condiciones\n",
    "paths = []\n",
    "values = []\n",
    "samples = []\n",
    "\n",
    "while nodos_pendientes:\n",
    "    nodo_actual, condiciones_actuales = nodos_pendientes.pop()\n",
    "\n",
    "    if tree_.feature[nodo_actual] != _tree.TREE_UNDEFINED:\n",
    "        idx_feature = tree_.feature[nodo_actual]\n",
    "        umbral = tree_.threshold[nodo_actual]\n",
    "        nombre_feature = feature_names_array[idx_feature]\n",
    "\n",
    "        # Rama izquierda: <=\n",
    "        cond_izq = condiciones_actuales + [f\"{nombre_feature} <= {umbral:.4f}\"]\n",
    "        nodos_pendientes.append((tree_.children_left[nodo_actual], cond_izq))\n",
    "\n",
    "        # Rama derecha: >\n",
    "        cond_der = condiciones_actuales + [f\"{nombre_feature} > {umbral:.4f}\"]\n",
    "        nodos_pendientes.append((tree_.children_right[nodo_actual], cond_der))\n",
    "\n",
    "    else:\n",
    "        # Nodo hoja\n",
    "        paths.append(condiciones_actuales)\n",
    "        values.append(tree_.value[nodo_actual][0][0])\n",
    "        samples.append(tree_.n_node_samples[nodo_actual])\n",
    "\n",
    "# Convertimos a DataFrame\n",
    "escenarios = pd.DataFrame({\n",
    "    'value': values,\n",
    "    'samples': samples,\n",
    "    'condiciones': paths\n",
    "}).sort_values(by='value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 0)  # o poné un número alto como 2000 si estás en VS Code\n",
    "display(escenarios)\n",
    "print(escenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vistazo rápido a los escenarios: solo filtro \n",
    "qtiles = escenarios['value'].quantile([0.33, 0.66]).values\n",
    "condiciones_clasificadas = []\n",
    "\n",
    "for i, row in escenarios.iterrows():\n",
    "    if row['value'] <= qtiles[0]:\n",
    "        condiciones_clasificadas.append('ventas bajas')\n",
    "    elif row['value'] <= qtiles[1]:\n",
    "        condiciones_clasificadas.append('ventas medias')\n",
    "    else:\n",
    "        condiciones_clasificadas.append('ventas altas')\n",
    "\n",
    "escenarios['categoria'] = condiciones_clasificadas\n",
    "\n",
    "# Agrupamos y mostramos el escenario promedio más representativo de cada clase\n",
    "resumen_escenarios = escenarios.groupby('categoria').apply(\n",
    "    lambda g: g.loc[g['samples'].idxmax()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(resumen_escenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una posibilidad más robusta? qué podríamos hacer para mejorar esto? \n",
    "\n",
    "\n",
    "- vamos a dividir en terciles los resultados del arbol para clasificar los escenarios de ventas en máximo, medio, mínimo, y vamos a tomar los valores medios de cada tercil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de los escenarios: \n",
    "\n",
    "# Primero clasificamos los nodos en categorías\n",
    "terciles = escenarios['value'].quantile([1/3, 2/3])\n",
    "umbral_bajo = terciles.iloc[0]\n",
    "umbral_medio = terciles.iloc[1]\n",
    "\n",
    "cond_min = escenarios['value'] <= umbral_bajo\n",
    "cond_med = (escenarios['value'] > umbral_bajo) & (escenarios['value'] <= umbral_medio)\n",
    "cond_max = escenarios['value'] > umbral_medio\n",
    "\n",
    "escenarios['categoria'] = None\n",
    "escenarios.loc[cond_max, 'categoria'] = 'alta'\n",
    "escenarios.loc[cond_med, 'categoria'] = 'media'\n",
    "escenarios.loc[cond_min, 'categoria'] = 'baja'\n",
    "\n",
    "# Para cada categoría, cálculo manual del promedio ponderado, min, max, desvío\n",
    "escenarios_alta = escenarios[escenarios['categoria'] == 'alta']\n",
    "total_samples_alta = escenarios_alta['samples'].sum()\n",
    "media_pond_alta = (escenarios_alta['value'] * escenarios_alta['samples']).sum() / total_samples_alta\n",
    "min_alta = escenarios_alta['value'].min()\n",
    "max_alta = escenarios_alta['value'].max()\n",
    "desvio_pond_alta = np.sqrt(((escenarios_alta['value'] - media_pond_alta) ** 2 * escenarios_alta['samples']).sum() / total_samples_alta)\n",
    "\n",
    "escenarios_media = escenarios[escenarios['categoria'] == 'media']\n",
    "total_samples_media = escenarios_media['samples'].sum()\n",
    "media_pond_media = (escenarios_media['value'] * escenarios_media['samples']).sum() / total_samples_media\n",
    "min_media = escenarios_media['value'].min()\n",
    "max_media = escenarios_media['value'].max()\n",
    "desvio_pond_media = np.sqrt(((escenarios_media['value'] - media_pond_media) ** 2 * escenarios_media['samples']).sum() / total_samples_media)\n",
    "\n",
    "escenarios_baja = escenarios[escenarios['categoria'] == 'baja']\n",
    "total_samples_baja = escenarios_baja['samples'].sum()\n",
    "media_pond_baja = (escenarios_baja['value'] * escenarios_baja['samples']).sum() / total_samples_baja\n",
    "min_baja = escenarios_baja['value'].min()\n",
    "max_baja = escenarios_baja['value'].max()\n",
    "desvio_pond_baja = np.sqrt(((escenarios_baja['value'] - media_pond_baja) ** 2 * escenarios_baja['samples']).sum() / total_samples_baja)\n",
    "\n",
    "# Construcción de tabla resumen\n",
    "resumen_escenarios = pd.DataFrame([\n",
    "    {\n",
    "        'categoria': 'alta',\n",
    "        'valor_esperado': media_pond_alta,\n",
    "        'min': min_alta,\n",
    "        'max': max_alta,\n",
    "        'desvio_ponderado': desvio_pond_alta,\n",
    "        'nodos': len(escenarios_alta),\n",
    "        'samples_total': total_samples_alta\n",
    "    },\n",
    "    {\n",
    "        'categoria': 'media',\n",
    "        'valor_esperado': media_pond_media,\n",
    "        'min': min_media,\n",
    "        'max': max_media,\n",
    "        'desvio_ponderado': desvio_pond_media,\n",
    "        'nodos': len(escenarios_media),\n",
    "        'samples_total': total_samples_media\n",
    "    },\n",
    "    {\n",
    "        'categoria': 'baja',\n",
    "        'valor_esperado': media_pond_baja,\n",
    "        'min': min_baja,\n",
    "        'max': max_baja,\n",
    "        'desvio_ponderado': desvio_pond_baja,\n",
    "        'nodos': len(escenarios_baja),\n",
    "        'samples_total': total_samples_baja\n",
    "    }\n",
    "])\n",
    "\n",
    "display(resumen_escenarios)\n",
    "print(resumen_escenarios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creamos columna auxiliar de categoría ya existente\n",
    "escenarios['categoria'] = pd.cut(\n",
    "    escenarios['value'],\n",
    "    bins=[-float('inf'), escenarios['value'].quantile(1/3), escenarios['value'].quantile(2/3), float('inf')],\n",
    "    labels=['baja', 'media', 'alta']\n",
    ")\n",
    "\n",
    "# Aplanamos condiciones por categoría\n",
    "paths_por_categoria = {'alta': [], 'media': [], 'baja': []}\n",
    "\n",
    "for _, row in escenarios.iterrows():\n",
    "    cat = row['categoria']\n",
    "    paths_por_categoria[cat] += row['condiciones']\n",
    "    \n",
    "\n",
    "\n",
    "for categoria, lista_de_condiciones in paths_por_categoria.items():\n",
    "    print(f\"\\n--- Condiciones frecuentes en categoría: {categoria} ---\")\n",
    "    conteo = Counter(lista_de_condiciones)\n",
    "    for condicion, frecuencia in conteo.most_common(10):  # top 10\n",
    "        print(f\"{condicion} → {frecuencia} veces\")\n",
    "\n",
    "\n",
    "for categoria in paths_por_categoria:\n",
    "    total_nodos_cat = escenarios[escenarios['categoria'] == categoria].shape[0]\n",
    "    print(f\"\\n--- Condiciones robustas en categoría: {categoria} ---\")\n",
    "    for cond, freq in Counter(paths_por_categoria[categoria]).items():\n",
    "        if freq / total_nodos_cat >= 0.5:\n",
    "            print(f\"{cond} → {freq} / {total_nodos_cat} nodos ({round(freq / total_nodos_cat * 100)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "                          +--------------------+\n",
    "                          |   Random Forest    |\n",
    "                          +--------------------+\n",
    "                                   |\n",
    "     +-----------------------------+-----------------------------+\n",
    "     |                             |                             |\n",
    "+------------+             +------------+               +------------+\n",
    "|  Árbol #1  |             |  Árbol #2  |       ...     | Árbol #100 | (n_estimators)\n",
    "+------------+             +------------+               +------------+\n",
    "     |                             |                             |\n",
    " Predicción y₁             Predicción y₂               Predicción y₁₀₀\n",
    "     \\\\                             |                             /\n",
    "      \\\\___________________________ | ___________________________/\n",
    "                                   ↓\n",
    "                      Promedio de todas las predicciones\n",
    "                                   ↓\n",
    "                     → Predicción final del modelo (ŷ)\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del Random Forest\n",
    "forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_rf = forest_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"\\n======== Random Forest Evaluation ========\")\n",
    "print(\"MSE del modelo:\", mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "# nRMSE\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "std_y_rf = np.std(y_test)\n",
    "nrmse_rf = rmse_rf / std_y_rf\n",
    "print(\"nRMSE:\", round(nrmse_rf, 4))\n",
    "\n",
    "# MASE\n",
    "mae_modelo_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mae_naive_rf = mean_absolute_error(y_test, np.full_like(y_test, np.mean(y_test)))\n",
    "mase_rf = mae_modelo_rf / mae_naive_rf\n",
    "print(\"MASE:\", round(mase_rf, 4))\n",
    "\n",
    "# SMAPE\n",
    "smape_rf = np.mean(\n",
    "    np.abs(y_test - y_pred_rf) / ((np.abs(y_test) + np.abs(y_pred_rf)) / 2)\n",
    ") * 100\n",
    "print(\"SMAPE (%):\", round(smape_rf, 2))\n",
    "\n",
    "# R²\n",
    "print(\"R²:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Importancia de variables (gain total estimado por el bosque)\n",
    "importancia_rf = pd.DataFrame({\n",
    "    'Variable': features,\n",
    "    'Importancia': forest_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\n--- Importancia de variables (Random Forest) ---\")\n",
    "print(importancia_rf)\n",
    "\n",
    "\n",
    "# Permutation Importance\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    forest_model, X_test, y_test, n_repeats=10, random_state=42\n",
    ")\n",
    "importancia_perm = pd.DataFrame({\n",
    "    'Variable': X_test.columns,\n",
    "    'Importancia': perm_importance.importances_mean\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\n--- Importancia por permutación (Permutation Importance) ---\")\n",
    "print(importancia_perm)\n",
    "\n",
    "\n",
    "\n",
    "# SHAP values para el Random Forest\n",
    "explainer_rf = shap.TreeExplainer(forest_model)\n",
    "shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "\n",
    "# Filtro de variables con impacto nulo\n",
    "shap_importancia_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "variables_con_impacto_rf = shap_importancia_rf > 0\n",
    "X_shap_rf = X_test.loc[:, variables_con_impacto_rf]\n",
    "features_shap_rf = X_shap_rf.columns.tolist()\n",
    "\n",
    "custom_cmap_rf = LinearSegmentedColormap.from_list(\"gray_to_red\", [\"#d3d3d3\", \"#ff0000\"])\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(\n",
    "    shap_values_rf[:, variables_con_impacto_rf],\n",
    "    X_shap_rf,\n",
    "    feature_names=features_shap_rf,\n",
    "    plot_size=(8, 5),\n",
    "    cmap=custom_cmap_rf,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Values Impact on Model Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo se interpretan los resultados del random forest en relación al decision tree? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Comparamos en relación a los resultados del tree. En este caso, vamos a ver que disminuyen los errores (MSE...SMAPE) y que aumenta el poder explicativo del modelo (R2)\n",
    "\n",
    "Por otro lado, al comparar las permutations, vemos cómo baja la performance explicativa de las variable clave, pero se mantiene el orden y el rango de importancia. \n",
    "\n",
    "\n",
    "### Indicadores estructurales\n",
    "\n",
    "\n",
    "| Indicador | Decision Tree | Random Forest | Mejora (Reduccción de Error) |\n",
    "|-----------|---------------|---------------|-----------------|\n",
    "| MSE | 234.97 | 127.44 | -45.8% |\n",
    "| nRMSE | 0.4138 | 0.3048 | -26.3% |\n",
    "| MASE | 0.3950 | 0.2976 | -24.7% |\n",
    "| SMAPE (%) | 11.81 | 8.87 | -24.9% |\n",
    "| R² | 0.8287 | 0.9071 | +7.84 pp |\n",
    "\n",
    "\n",
    "### Importancia de variables por permutación\n",
    "\n",
    "| Variable | Perm. Importance (DT) | Perm. Importance (RF) |\n",
    "|----------|----------------------|----------------------|\n",
    "| Promocion_Croissant | 0.9298 | 0.8836 |\n",
    "| Es_Fin_Semana | 0.3939 | 0.3799 |\n",
    "| Temperatura_Max_Prevista | 0.2790 | 0.2552 |\n",
    "| Es_Feriado | 0.1076 | 0.1152 |\n",
    "| Dia_Semana_viernes | 0.0179 | 0.0579 |\n",
    "| Precio_Nuestro | 0.0073 | 0.0363 |\n",
    "| Precio_Competencia | 0.0000 | 0.0070 |\n",
    "| Resto de variables | 0.0000 | < 0.005 (todas) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seguimos el próximo encuentro. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
